{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# NOTE: you CAN change this cell\n",
    "# If you want to use your own database, download it here\n",
    "# !gdown ...\n",
    "!rm -rf list_province.txt\n",
    "!rm -rf list_district.txt\n",
    "!rm -rf list_ward.txt\n",
    "\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1FC5Kb1iL_ElLboQ_yuJt-RyujZWVBe5N/view?usp=sharing -O list_province.txt\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1mLEuPqn_01ffMGnDy2fntHhpWSNyYHSW/view?usp=sharing -O list_district.txt\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1LmCBNSqf2fY4lcphAFgQlMR_GkCwSLkk/view?usp=sharing -O list_ward.txt"
   ],
   "metadata": {
    "id": "i20WfB6lqiUy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "209c7e71-8c91-4802-f702-11b8f7b2ab57"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FC5Kb1iL_ElLboQ_yuJt-RyujZWVBe5N\n",
      "To: /content/list_province.txt\n",
      "100% 5.07k/5.07k [00:00<00:00, 18.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mLEuPqn_01ffMGnDy2fntHhpWSNyYHSW\n",
      "To: /content/list_district.txt\n",
      "100% 4.72k/4.72k [00:00<00:00, 16.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LmCBNSqf2fY4lcphAFgQlMR_GkCwSLkk\n",
      "To: /content/list_ward.txt\n",
      "100% 4.79k/4.79k [00:00<00:00, 16.5MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "  # NOTE: you CAN change this cell\n",
    "# Add more to your needs\n",
    "# you must place ALL pip install here\n",
    "!pip install editdistance"
   ],
   "metadata": {
    "id": "J8znFuZTzwoS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55949e7e-a0ab-463b-b08b-3d8cebaf3947"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# NOTE: you CAN change this cell\n",
    "# import your library here\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from queue import Queue\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import editdistance\n",
    "from gettext import textdomain\n",
    "from numpy import character\n"
   ],
   "metadata": {
    "id": "AodaIxYa32hT"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "id": "QAsaktzUnFy5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from gettext import textdomain\n",
    "\n",
    "import unicodedata\n",
    "from numpy import character\n",
    "\n",
    "SAFE_CASES = [\n",
    "                #\"TP.\", \"T.P\", \"F.\", \"Thành phố\", \"ThànhPhố\", \"TP \", \" TP\",\n",
    "               \"Tỉnh\", \" T \", #\"Tỉn\",  \",T \", \"T.\",\n",
    "               #\"Quận\", \"Q.\", \" Q \", \",Q \", -> quận 5 sau khi remove sẽ thành 5 và không tìm ra\n",
    "               \"Huyện\", \"hyện\", #\"H.\", \" H \", \",H \",\n",
    "               # \"Phường\", \"F\",  \"P.\", \" P \", \",P \",\n",
    "               \"Thị xã\", \"ThịXã\", \"Xã\", # \"X.\", \" X \", \"X \", \",X \",\n",
    "                \"Thị trấn\", \"ThịTrấn\", #\"T.T\",\n",
    "                \"khu phố\", \"KP\", \"KhuPhố\", \"Khu pho\", \"KhuPho\", # -> KP5 bị nhầm thành P5\n",
    "                \"Ấp\"\n",
    "               ]\n",
    "\n",
    "\n",
    "ALL_PREFIXES = [\"TP.\", \"T.P\", \"F.\", \"Thành phố\", \"ThànhPhố\", \"TP \", \" TP\",\n",
    "               \"Tỉnh\", \"Tỉn\",\"T.\", \" T \",  #\",T \",\n",
    "               \"Quận\", \"Q.\", \" Q \", \",Q \",\n",
    "               \"Huyện\", \"hyện\", \"H.\", \" H \",  #\",H \",\n",
    "                #\"KP\", # KP. contains P.\n",
    "               \"khu phố\",  \"KhuPhố\", \"Khu pho\", \"KhuPho\",\n",
    "               \"Phường\", \"P.\", \"F\", \" P \",  #\",P \",\n",
    "               \"X.\", \"Thị xã\", \"ThịXã\", \"Xã\",  #\" X \", \"X \", \",X \",\n",
    "                \"Thị trấn\", \"ThịTrấn\", \"T.T\",\n",
    "               \"-\"\n",
    "                ]\n",
    "\n",
    "vietnamese_dict = {\n",
    "    \"a\": [\"à\", \"á\", \"ạ\", \"ả\", \"ã\", \"â\", \"ầ\", \"ấ\", \"ậ\", \"ẩ\", \"ẫ\", \"ă\", \"ằ\", \"ắ\", \"ặ\", \"ẳ\", \"ẵ\"],\n",
    "    \"d\": [\"đ\"],\n",
    "    \"e\": [\"è\", \"é\", \"ẹ\", \"ẻ\", \"ẽ\", \"ê\", \"ề\", \"ế\", \"ệ\", \"ể\", \"ễ\"],\n",
    "    \"i\": [\"ì\", \"í\", \"ị\", \"ỉ\", \"ĩ\"],\n",
    "    \"o\": [\"ò\", \"ó\", \"ọ\", \"ỏ\", \"õ\", \"ô\", \"ồ\", \"ố\", \"ộ\", \"ổ\", \"ỗ\", \"ơ\", \"ờ\", \"ớ\", \"ợ\", \"ở\", \"ỡ\"],\n",
    "    \"u\": [\"ù\", \"ú\", \"ụ\", \"ủ\", \"ũ\", \"ư\", \"ừ\", \"ứ\", \"ự\", \"ử\", \"ữ\"],\n",
    "    \"y\": [\"ỳ\", \"ý\", \"ỵ\", \"ỷ\", \"ỹ\"],\n",
    "\n",
    "}\n",
    "\n",
    "reversed_vietnamese_dict = {}\n",
    "\n",
    "wrong_accents = {\n",
    "    \"oà\": \"òa\", \"oá\": \"óa\", \"oạ\": \"ọa\", \"oã\": \"õa\", \"oả\": \"ỏa\",\n",
    "    \"qui\": \"quy\",\n",
    "}\n",
    "\n",
    "def common_normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # text = text.replace(\",\", \"\")  # replace for T,â,n,B,ì,n,h Dĩ An Bình Dương\n",
    "    # text = text.replace(\".\", \"\")\n",
    "    for case in SAFE_CASES:\n",
    "        text = re.sub(re.escape(case), ',', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \",\", text)  # Remove spaces\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text\n",
    "\n",
    "def normalize_text_and_remove_accent(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "    text = common_normalize(text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.replace(\"đ\", \"d\")\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "    # text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "    return text\n",
    "\n",
    "def normalize_text_but_keep_vietnamese_alphabet(text: str) -> str:\n",
    "    text = common_normalize(text)\n",
    "    for base_char, variations in vietnamese_dict.items():\n",
    "        for char in variations:\n",
    "            reversed_vietnamese_dict[char] = base_char\n",
    "\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in reversed_vietnamese_dict:\n",
    "            result.append(reversed_vietnamese_dict[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "\n",
    "    text = \"\".join(result)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "    text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "    return text\n",
    "\n",
    "def normalize_text_but_keep_accent(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "    text = common_normalize(text)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    for wrong, correct in wrong_accents.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "\n",
    "    return text\n",
    "\n",
    "def segment_text(s, safe=True):\n",
    "    text = s[:]\n",
    "\n",
    "    prefixes = SAFE_CASES if safe else ALL_PREFIXES\n",
    "\n",
    "    for p in prefixes:\n",
    "        text = re.sub(re.escape(p), ',', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Xử lý dấu \"-\" trong tên (ví dụ: \"Ng-T-\" -> \"Ng T \")\n",
    "    text = re.sub(r'[.\\-]', ' ', text)\n",
    "\n",
    "    # Tách cụm địa chỉ\n",
    "    segments = [seg.strip() for seg in text.split(',') if seg.strip()]\n",
    "    #\n",
    "    # print()\n",
    "    # print(f\"'{s}'  -->  {segments}\")\n",
    "    return segments\n",
    "\n",
    "\n",
    "# print(normalize_text_but_keep_accent(\"T18,Cẩm Bình, Cẩm Phả, Quảng Ninh\"))\n",
    "# print(normalize_text_but_keep_vietnamese(\"Thôn Đồng Lực Hoàng Lâu, Tam Dương, Vĩnh Phúc\"))\n",
    "# print(normalize_text_but_keep_vietnamese(\"Tam Đường, Tam Đường, Lai Châu\"))\n",
    "\n"
   ],
   "metadata": {
    "id": "qTASEb_BnHs4"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Index Analyzer"
   ],
   "metadata": {
    "id": "mBBRJNQLmwNc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from queue import Queue\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# from Utils import *\n",
    "locality_map = {}\n",
    "current_id = 0\n",
    "\n",
    "# SPECIAL_CASES = [\"xã\", \"x.\", \"huyện\", \"tỉnh\", \"t.\",\n",
    "#                  \"tp\", \"thành phố\", \"thànhphố\"]\n",
    "\n",
    "# Prefixes for wards and districts to expand possible matches\n",
    "# DIGIT_CASES = {\n",
    "#     \"ward\": [\"p\", \"phường\"],\n",
    "#     \"district\": [\"q\", \"quận\"],\n",
    "# }\n",
    "SPECIAL_CASES = [\"xã\", \"x.\", \"huyện\", \"tỉnh\", \"t.\", \"số nhà\", \"thị trấn\", \"ấp\",\n",
    "                 \"h.\", \"khu phố\",\"tp\", \"thành phố\", \"thànhphố\"]\n",
    "WARD_CASES = [\"p\", \"p.\", \"phường\"]\n",
    "DISTRICT_CASES = [\"q\", \"q.\", \"quận\"]\n",
    "\n",
    "# Dictionary to store generated variations for tracing back\n",
    "variation_map: Dict[str, dict] = {}\n",
    "\n",
    "province_short_form = {\n",
    "    \"hồchíminh\":\"hcm\",\n",
    "    \"bàrịavũngtàu\":\"brvt\",\n",
    "}\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.original_string: Optional[str] = None\n",
    "\n",
    "\n",
    "class Trie:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        self.all_words = set()\n",
    "        self.original_names: Dict[str, str] = {} #normalize -> raw\n",
    "\n",
    "    def insert(self, normalized_word: str):\n",
    "        \"\"\"Insert a normalized word into the trie with a reference to the original.\"\"\"\n",
    "        node = self.root\n",
    "        for i, char in enumerate(normalized_word):\n",
    "\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "\n",
    "            node = node.children[char]\n",
    "\n",
    "        node.is_end_of_word = True\n",
    "        node.original_string = normalized_word\n",
    "\n",
    "        self.all_words.add(normalized_word)\n",
    "\n",
    "    def search(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "        \"\"\"Finds the first valid word from the given start index.\"\"\"\n",
    "        node = self.root\n",
    "        for i in range(start, len(text)):\n",
    "            char = text[i]\n",
    "            if char not in node.children:\n",
    "                break\n",
    "            node = node.children[char]\n",
    "            if node.is_end_of_word:\n",
    "                return (node.original_string, start, i + 1)\n",
    "        return None\n",
    "\n",
    "    def search_max_length(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "        \"\"\"Finds the longest valid word from the given start index.\"\"\"\n",
    "        node = self.root\n",
    "        longest_match = \"\"\n",
    "        longest_length = 0\n",
    "        for i in range(start, len(text)):\n",
    "            char = text[i]\n",
    "            if char not in node.children:\n",
    "                break\n",
    "            node = node.children[char]\n",
    "            if node.is_end_of_word:\n",
    "                current_length = i - start + 1\n",
    "                if current_length > longest_length:\n",
    "                    longest_length = current_length\n",
    "                    longest_match = (node.original_string, start, i + 1)\n",
    "        return longest_match\n",
    "\n",
    "    def get_raw_text(self, normalized_text):\n",
    "        return self.original_names.get(normalized_text, normalized_text)\n",
    "\n",
    "def generate_prefixed_variations(location_name: str, category: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Generate prefixed variations ONLY for wards and districts, and store variations per category.\"\"\"\n",
    "    variations = []\n",
    "\n",
    "    if category not in variation_map:\n",
    "        variation_map[category] = {}\n",
    "\n",
    "    normalized_name = normalize_text_but_keep_accent(location_name)\n",
    "\n",
    "    if normalized_name.isdigit():  # Only generate prefixes for wards and districts\n",
    "        variations = [prefix + normalized_name for prefix in DIGIT_CASES[category]]\n",
    "    elif normalized_name in province_short_form:\n",
    "        variations = [normalized_name, province_short_form[normalized_name]]\n",
    "    else:\n",
    "        variations = [normalized_name]\n",
    "\n",
    "    non_accents_variations = [normalize_text_and_remove_accent(variation) for variation in variations]\n",
    "    variations.extend(non_accents_variations)\n",
    "\n",
    "    # Store variations per category\n",
    "    variation_map[category][normalized_name] = variations\n",
    "    return variations, normalized_name\n",
    "\n",
    "\n",
    "def load_databases(filenames: Dict[str, str], tries) -> Dict[str, Trie]:\n",
    "    \"\"\"Load multiple database files into separate Tries with prefixed variations.\"\"\"\n",
    "    for category, filename in filenames.items():\n",
    "        trie = Trie()\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    load_line(line, trie, category)\n",
    "            tries[category] = trie\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {filename} not found!\")\n",
    "            tries[category] = Trie()\n",
    "    return tries\n",
    "\n",
    "def load_line(line, trie, category):\n",
    "    location_name = line.strip()\n",
    "    if location_name == \"\":\n",
    "        return\n",
    "\n",
    "    prefixed_variations, normalized_text = generate_prefixed_variations(location_name, category)\n",
    "\n",
    "    for variant in prefixed_variations:\n",
    "        trie.original_names[variant] = location_name\n",
    "        trie.insert(variant)\n"
   ],
   "metadata": {
    "id": "ypVaUDchmhvc"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrect"
   ],
   "metadata": {
    "id": "TMAkHGhQnXkr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Both Normalized and Un-normalized words are loaded to Trie\n",
    "# Inputs are misspelled words (misspelled_words)\n",
    "# Outputs are corrected words via Database\n",
    "# If Output is 'null', please check Database if it really exists\n",
    "# Two parameters can be tuned: COSINE_SIMILARITY_THRESHOLD = 0.75\n",
    "#                                     distance = damerau_levenshtein(word_normalized, candidate_normalized, max_distance=3)\n",
    "# Search Priority is set as province > district > ward\n",
    "\n",
    "####################################################################\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# from IndexAnalyzer import Trie\n",
    "# from Utils import normalize_text_but_keep_vietnamese_alphabet, normalize_text_but_keep_accent\n",
    "\n",
    "# Define category ranking\n",
    "CATEGORY_PRIORITY = {\"province\": 1, \"district\": 2, \"ward\": 3}  # Lower number = higher priority\n",
    "COSINE_SIMILARITY_THRESHOLD = 0.75  # If similarity < 0.7, return \"null\", the less value - the less strict\n",
    "MAX_VALID_EDIT_DISTANCE = 3\n",
    "\n",
    "# Damerau-Levenshtein Distance with max_distance\n",
    "# def damerau_levenshtein(s1, s2, max_distance=3):\n",
    "#     len_s1, len_s2 = len(s1), len(s2)\n",
    "#     if abs(len_s1 - len_s2) > max_distance:\n",
    "#         return max_distance + 1\n",
    "#\n",
    "#     prev_row = list(range(len_s2 + 1))\n",
    "#     curr_row = [0] * (len_s2 + 1)\n",
    "#\n",
    "#     for i in range(1, len_s1 + 1):\n",
    "#         curr_row[0] = i\n",
    "#         min_row_value = i\n",
    "#\n",
    "#         for j in range(1, len_s2 + 1):\n",
    "#             cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "#             curr_row[j] = min(curr_row[j - 1] + 1, prev_row[j] + 1, prev_row[j - 1] + cost)\n",
    "#             if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "#                 curr_row[j] = min(curr_row[j], prev_row[j - 2] + 1)\n",
    "#\n",
    "#             min_row_value = min(min_row_value, curr_row[j])\n",
    "#\n",
    "#         if min_row_value > max_distance:\n",
    "#             return max_distance + 1\n",
    "#\n",
    "#         prev_row, curr_row = curr_row, prev_row\n",
    "#\n",
    "#     return prev_row[len_s2]\n",
    "\n",
    "# Cosine Similarity Function\n",
    "def cosine_similarity(s1, s2):\n",
    "    vec1, vec2 = Counter(s1), Counter(s2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    dot_product = sum(vec1[x] * vec2[x] for x in intersection)\n",
    "    magnitude1 = math.sqrt(sum(vec1[x] ** 2 for x in vec1.keys()))\n",
    "    magnitude2 = math.sqrt(sum(vec2[x] ** 2 for x in vec2.keys()))\n",
    "    return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
    "\n",
    "# Autocorrection with Category Priority, Damerau-Levenshtein Distance & Cosine Similarity Check\n",
    "def autocorrect(word_normalized, trie: Trie, category):\n",
    "    best_distance = float(\"inf\")\n",
    "    matches = []\n",
    "    for candidate_normalized in trie.all_words:\n",
    "        distance = editdistance.distance(word_normalized, candidate_normalized)\n",
    "\n",
    "        # Prioritize lower edit distance & higher-ranked category (Province > District > Ward)\n",
    "        if distance < min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "            matches = [candidate_normalized]\n",
    "            best_distance = distance\n",
    "        elif distance == min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "            matches.append(candidate_normalized)\n",
    "\n",
    "    # Check Cosine Similarity Threshold\n",
    "    best_match = \"\"\n",
    "    best_similarity = 0\n",
    "    for match in matches:\n",
    "        p = cosine_similarity(word_normalized, match)\n",
    "        if p > COSINE_SIMILARITY_THRESHOLD and  p > best_similarity:\n",
    "            best_similarity = p\n",
    "            best_match = match\n",
    "\n",
    "    return best_match\n"
   ],
   "metadata": {
    "id": "rI1EQ9YAnaFC"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Searcher"
   ],
   "metadata": {
    "id": "XBpTj3Apm3EV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# from IndexAnalyzer import Trie\n",
    "# from Autocorrect import autocorrect\n",
    "# from Utils import normalize_text_and_remove_accent\n",
    "\n",
    "\n",
    "def search_locations_in_trie(tries: Dict[str, Trie], input_text: str, results) -> Tuple[Dict[str, Optional[str]], str]:\n",
    "    matched_positions = set()\n",
    "\n",
    "    remaining_chars = list(input_text)\n",
    "\n",
    "    for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "        if results[category] != \"\":\n",
    "            continue\n",
    "        match = search_part(tries[category], input_text, matched_positions, remaining_chars, reverse)\n",
    "        res = match[0]\n",
    "        if match and res:\n",
    "            input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "        results[category] = res\n",
    "\n",
    "    # remaining_text = normalize_text_and_remove_accent(input_text)\n",
    "    # for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "    #     if results[category] != \"\":\n",
    "    #         continue\n",
    "    #     match = search_part(tries[category], remaining_text, matched_positions, remaining_chars, reverse)\n",
    "    #     res = match[0]\n",
    "    #     if match and res:\n",
    "    #         remaining_text = remaining_text[:match[1]] + \",\" + remaining_text[match[2]:]\n",
    "    #         input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "    #     results[category] = res\n",
    "\n",
    "    return results, input_text\n",
    "\n",
    "def search_locations_in_segments(tries: Dict[str, Trie], segments: [], results) -> Tuple[Dict[str, Optional[str]], str]:\n",
    "    for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "        if results[category] != \"\":\n",
    "            continue\n",
    "\n",
    "        res = search_in_segment(segments, tries[category], category, reverse)\n",
    "        results[category] = res\n",
    "\n",
    "    return results, segments\n",
    "\n",
    "def search_in_trie(trie, input_text, matched_positions, remaining_chars, reverse):\n",
    "    match = search_part(trie, input_text, matched_positions, remaining_chars, reverse)\n",
    "    res = match[0]\n",
    "    if match and res:\n",
    "        input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "    return res, input_text\n",
    "\n",
    "def search_in_segment(segments, trie, category, reverse=False):\n",
    "    if reverse:\n",
    "        segments.reverse()\n",
    "\n",
    "    for seg in segments:\n",
    "        word = autocorrect(seg, trie, category)\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        segments.remove(seg)\n",
    "        return word\n",
    "\n",
    "    if reverse:\n",
    "        segments.reverse()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def search_part(trie, input_text, matched_positions, remaining_chars, reversed=False):\n",
    "    it_range = range(len(input_text))\n",
    "    if reversed:\n",
    "        it_range = it_range.__reversed__()\n",
    "    for i in it_range:\n",
    "        if i in matched_positions:\n",
    "            continue\n",
    "        match = trie.search_max_length(input_text, i)\n",
    "        if match:\n",
    "            return match\n",
    "    return \"\", None, None"
   ],
   "metadata": {
    "id": "ujQ0od5Mm9AC"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution"
   ],
   "metadata": {
    "id": "ImCuX98Mm-JQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # NOTE: you MUST change this cell\n",
    "# # New methods / functions must be written under class Solution.\n",
    "# class TrieNode:\n",
    "#     def __init__(self):\n",
    "#         self.children = {}\n",
    "#         self.is_end_of_word = False\n",
    "#         self.original_string: Optional[str] = None\n",
    "\n",
    "# class Trie:\n",
    "#     def __init__(self):\n",
    "#         self.root = TrieNode()\n",
    "#         self.all_words = set()\n",
    "#         self.original_names: Dict[str, str] = {}\n",
    "\n",
    "#     def insert(self, normalized_word: str):\n",
    "#         \"\"\"Insert a normalized word into the trie with a reference to the original.\"\"\"\n",
    "#         node = self.root\n",
    "#         for i, char in enumerate(normalized_word):\n",
    "#             if char not in node.children:\n",
    "#                 node.children[char] = TrieNode()\n",
    "#             node = node.children[char]\n",
    "#         node.is_end_of_word = True\n",
    "#         node.original_string = normalized_word\n",
    "#         self.all_words.add(normalized_word)\n",
    "\n",
    "#     def search(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "#         \"\"\"Finds the first valid word from the given start index.\"\"\"\n",
    "#         node = self.root\n",
    "#         for i in range(start, len(text)):\n",
    "#             char = text[i]\n",
    "#             if char not in node.children:\n",
    "#                 break\n",
    "#             node = node.children[char]\n",
    "#             if node.is_end_of_word:\n",
    "#                 return (node.original_string, start, i + 1)\n",
    "#         return None\n",
    "\n",
    "#     def search_max_length(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "#         \"\"\"Finds the longest valid word from the given start index.\"\"\"\n",
    "#         node = self.root\n",
    "#         longest_match = \"\"\n",
    "#         longest_length = 0\n",
    "#         for i in range(start, len(text)):\n",
    "#             char = text[i]\n",
    "#             if char not in node.children:\n",
    "#                 break\n",
    "#             node = node.children[char]\n",
    "#             if node.is_end_of_word:\n",
    "#                 current_length = i - start + 1\n",
    "#                 if current_length > longest_length:\n",
    "#                     longest_length = current_length\n",
    "#                     longest_match = (node.original_string, start, i + 1)\n",
    "#         return longest_match\n",
    "\n",
    "#     def get_raw_text(self, normalized_text):\n",
    "#         return self.original_names.get(normalized_text, normalized_text)\n",
    "\n",
    "\n",
    "# # ========================= Solution.py =========================\n",
    "# class Solution:\n",
    "#     SAFE_CASES = [\n",
    "#             \"Tỉnh\", \" T \",\n",
    "#             \"Huyện\", \"hyện\",\n",
    "#             \"Thị xã\", \"ThịXã\", \"Xã\",\n",
    "#             \"Thị trấn\", \"ThịTrấn\",\n",
    "#             \"khu phố\", \"KP\", \"KhuPhố\", \"Khu pho\", \"KhuPho\",\n",
    "#         ]\n",
    "\n",
    "#     ALL_PREFIXES = [\"TP.\", \"T.P\", \"F.\", \"Thành phố\", \"ThànhPhố\", \"TP \", \" TP\",\n",
    "#                                 \"Tỉnh\", \"Tỉn\", \"T.\", \" T \",\n",
    "#                                 \"Quận\", \"Q.\", \" Q \", \",Q \",\n",
    "#                                 \"Huyện\", \"hyện\", \"H.\", \" H \",\n",
    "#                                 \"khu phố\", \"KhuPhố\", \"Khu pho\", \"KhuPho\",\n",
    "#                                 \"Phường\", \"P.\", \"F\", \" P \",\n",
    "#                                 \"X.\", \"Thị xã\", \"ThịXã\", \"Xã\",\n",
    "#                                 \"Thị trấn\", \"ThịTrấn\", \"T.T\",\n",
    "#                                 \"-\"\n",
    "#                                 ]\n",
    "#     vietnamese_dict = {\n",
    "#                 \"a\": [\"à\", \"á\", \"ạ\", \"ả\", \"ã\", \"â\", \"ầ\", \"ấ\", \"ậ\", \"ẩ\", \"ẫ\", \"ă\", \"ằ\", \"ắ\", \"ặ\", \"ẳ\", \"ẵ\"],\n",
    "#                 \"d\": [\"đ\"],\n",
    "#                 \"e\": [\"è\", \"é\", \"ẹ\", \"ẻ\", \"ẽ\", \"ê\", \"ề\", \"ế\", \"ệ\", \"ể\", \"ễ\"],\n",
    "#                 \"i\": [\"ì\", \"í\", \"ị\", \"ỉ\", \"ĩ\"],\n",
    "#                 \"o\": [\"ò\", \"ó\", \"ọ\", \"ỏ\", \"õ\", \"ô\", \"ồ\", \"ố\", \"ộ\", \"ổ\", \"ỗ\", \"ơ\", \"ờ\", \"ớ\", \"ợ\", \"ở\", \"ỡ\"],\n",
    "#                 \"u\": [\"ù\", \"ú\", \"ụ\", \"ủ\", \"ũ\", \"ư\", \"ừ\", \"ứ\", \"ự\", \"ử\", \"ữ\"],\n",
    "#                 \"y\": [\"ỳ\", \"ý\", \"ỵ\", \"ỷ\", \"ỹ\"],\n",
    "#             }\n",
    "\n",
    "#     wrong_accents = {\n",
    "#                 \"oà\": \"òa\", \"oá\": \"óa\", \"oạ\": \"ọa\", \"oã\": \"õa\", \"oả\": \"ỏa\",\n",
    "#                 \"qui\": \"quy\",\n",
    "#             }\n",
    "\n",
    "#     SPECIAL_CASES = [\"xã\", \"x.\", \"huyện\", \"tỉnh\", \"t.\",\n",
    "#                     \"tp\", \"thành phố\", \"thànhphố\"]\n",
    "\n",
    "#     # Prefixes for wards and districts to expand possible matches\n",
    "#     DIGIT_CASES = {\n",
    "#         \"ward\": [\"p\", \"phường\"],\n",
    "#         \"district\": [\"q\", \"quận\"],\n",
    "#     }\n",
    "\n",
    "#     # Dictionary to store generated variations for tracing back\n",
    "#     variation_map: Dict[str, dict] = {}\n",
    "\n",
    "#     province_short_form = {\n",
    "#         \"hồchíminh\":\"hcm\",\n",
    "#         \"bàrịavũngtàu\":\"brvt\",\n",
    "#     }\n",
    "\n",
    "#     CATEGORY_PRIORITY = {\"province\": 1, \"district\": 2, \"ward\": 3}  # Lower number = higher priority\n",
    "#     COSINE_SIMILARITY_THRESHOLD = 0.75  # If similarity < 0.7, return \"null\", the less value - the less strict\n",
    "#     MAX_VALID_EDIT_DISTANCE = 3\n",
    "#     def __init__(self):\n",
    "#         # List for province, district, ward for private test, do not change for any reason (these files will be provided later with this exact name)\n",
    "#         self.province_path = 'list_province.txt'\n",
    "#         self.district_path = 'list_district.txt'\n",
    "#         self.ward_path = 'list_ward.txt'\n",
    "\n",
    "#         self.tries = {}\n",
    "#         self.variation_map = {}\n",
    "#         self.locality_map = {}\n",
    "#         self.current_id = 0\n",
    "#         self.reversed_vietnamese_dict = {}\n",
    "\n",
    "\n",
    "#         self.load_databases({\n",
    "#             \"province\": self.province_path,\n",
    "#             \"district\": self.district_path,\n",
    "#             \"ward\": self.ward_path\n",
    "#         })\n",
    "\n",
    "#     def common_normalize(self, text: str) -> str:\n",
    "#         text = text.lower()\n",
    "#         for case in self.SAFE_CASES:\n",
    "#             text = re.sub(re.escape(case), ',', text, flags=re.IGNORECASE)\n",
    "#         text = re.sub(r\"\\s{2,}\", \",\", text)  # Remove spaces\n",
    "#         text = text.replace(\" \", \"\")\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_and_remove_accent(self, text: str) -> str:\n",
    "#         \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "#         text = self.common_normalize(text)\n",
    "#         text = unicodedata.normalize(\"NFKD\", text)\n",
    "#         text = text.replace(\"đ\", \"d\")\n",
    "#         text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_but_keep_vietnamese_alphabet(self, text: str) -> str:\n",
    "#         text = self.common_normalize(text)\n",
    "#         for base_char, variations in vietnamese_dict.items():\n",
    "#             for char in variations:\n",
    "#                 self.reversed_vietnamese_dict[char] = base_char\n",
    "\n",
    "#         result = []\n",
    "#         for char in text:\n",
    "#             if char in self.reversed_vietnamese_dict:\n",
    "#                 result.append(self.reversed_vietnamese_dict[char])\n",
    "#             else:\n",
    "#                 result.append(char)\n",
    "\n",
    "#         text = \"\".join(result)\n",
    "#         text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "#         text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "#         text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_but_keep_accent(self, text: str) -> str:\n",
    "#         \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "#         text = self.common_normalize(text)\n",
    "#         text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "#         for wrong, correct in wrong_accents.items():\n",
    "#             text = text.replace(wrong, correct)\n",
    "\n",
    "#         return text\n",
    "\n",
    "#     def segment_text(self, s, safe=True):\n",
    "#         text = s[:]\n",
    "\n",
    "#         prefixes = self.SAFE_CASES if safe else self.ALL_PREFIXES\n",
    "\n",
    "#         for p in prefixes:\n",
    "#             text = re.sub(re.escape(p), ',', text, flags=re.IGNORECASE)\n",
    "\n",
    "#         # Handle \"-\" in names (e.g., \"Ng-T-\" -> \"Ng T \")\n",
    "#         text = re.sub(r'[.\\-]', ' ', text)\n",
    "\n",
    "#         # Split address components\n",
    "#         segments = [seg.strip() for seg in text.split(',') if seg.strip()]\n",
    "#         return segments\n",
    "\n",
    "#     def generate_prefixed_variations(self, location_name: str, category: str) -> Tuple[List[str], str]:\n",
    "#         \"\"\"Generate prefixed variations ONLY for wards and districts, and store variations per category.\"\"\"\n",
    "#         variations = []\n",
    "\n",
    "#         if category not in self.variation_map:\n",
    "#           self.variation_map[category] = {}\n",
    "\n",
    "#         normalized_name = self.normalize_text_but_keep_accent(location_name)\n",
    "\n",
    "#         if normalized_name.isdigit():  # Only generate prefixes for wards and districts\n",
    "#             variations = [prefix + normalized_name for prefix in self.DIGIT_CASES[category]]\n",
    "#         elif normalized_name in province_short_form:\n",
    "#             variations = [normalized_name, province_short_form[normalized_name]]\n",
    "#         else:\n",
    "#             variations = [normalized_name]\n",
    "\n",
    "#         non_accents_variations = [self.normalize_text_and_remove_accent(variation) for variation in variations]\n",
    "#         variations.extend(non_accents_variations)\n",
    "\n",
    "#         # Store variations per category\n",
    "#         self.variation_map[category][normalized_name] = variations\n",
    "#         return variations, normalized_name\n",
    "\n",
    "#     def load_databases(self, filenames: Dict[str, str]) -> None:\n",
    "#         \"\"\"Load multiple database files into separate Tries with prefixed variations.\"\"\"\n",
    "#         for category, filename in filenames.items():\n",
    "#             trie = Trie()\n",
    "#             try:\n",
    "#                 with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "#                     for line in file:\n",
    "#                         self.load_line(line, trie, category)\n",
    "#                 self.tries[category] = trie\n",
    "#             except FileNotFoundError:\n",
    "#                 print(f\"Warning: File {filename} not found!\")\n",
    "#                 self.tries[category] = Trie()\n",
    "\n",
    "#     def load_line(self, line, trie, category):\n",
    "#         location_name = line.strip()\n",
    "#         if location_name == \"\":\n",
    "#             return\n",
    "\n",
    "#         prefixed_variations, normalized_text = self.generate_prefixed_variations(location_name, category)\n",
    "\n",
    "#         for variant in prefixed_variations:\n",
    "#             trie.original_names[variant] = location_name\n",
    "#             trie.insert(variant)\n",
    "\n",
    "#     # Cosine Similarity Function\n",
    "#     def cosine_similarity(self,s1, s2):\n",
    "#         vec1, vec2 = Counter(s1), Counter(s2)\n",
    "#         intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "#         dot_product = sum(vec1[x] * vec2[x] for x in intersection)\n",
    "#         magnitude1 = math.sqrt(sum(vec1[x] ** 2 for x in vec1.keys()))\n",
    "#         magnitude2 = math.sqrt(sum(vec2[x] ** 2 for x in vec2.keys()))\n",
    "#         return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
    "\n",
    "#         # Autocorrection with Category Priority, Damerau-Levenshtein Distance & Cosine Similarity Check\n",
    "#     def autocorrect(self, word_normalized, trie, category):\n",
    "#         best_distance = float(\"inf\")\n",
    "#         matches = []\n",
    "#         for candidate_normalized in trie.all_words:\n",
    "#             distance = editdistance.distance(word_normalized, candidate_normalized)\n",
    "\n",
    "#             # Prioritize lower edit distance & higher-ranked category (Province > District > Ward)\n",
    "#             if distance < min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "#                 matches = [candidate_normalized]\n",
    "#                 best_distance = distance\n",
    "#             elif distance == min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "#                 matches.append(candidate_normalized)\n",
    "\n",
    "#         # Check Cosine Similarity Threshold\n",
    "#         best_match = \"\"\n",
    "#         best_similarity = 0\n",
    "#         for match in matches:\n",
    "#             p = self.cosine_similarity(word_normalized, match)\n",
    "#             if p > COSINE_SIMILARITY_THRESHOLD and p > best_similarity:\n",
    "#                 best_similarity = p\n",
    "#                 best_match = match\n",
    "\n",
    "#         return best_match\n",
    "\n",
    "#     def search_locations_in_trie(self, tries, input_text, results):\n",
    "#         matched_positions = set()\n",
    "#         remaining_chars = list(input_text)\n",
    "\n",
    "#         for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "#             if results[category] != \"\":\n",
    "#                 continue\n",
    "#             match = self.search_part(tries[category], input_text, matched_positions, remaining_chars, reverse)\n",
    "#             res = match[0]\n",
    "#             if match and res:\n",
    "#                 input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "#             results[category] = res\n",
    "\n",
    "#         return results, input_text\n",
    "\n",
    "#     def search_locations_in_segments(self, tries, segments, results):\n",
    "#         for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "#             if results[category] != \"\":\n",
    "#                 continue\n",
    "\n",
    "#             res = self.search_in_segment(segments, tries[category], category, reverse)\n",
    "#             results[category] = res\n",
    "\n",
    "#         return results, segments\n",
    "\n",
    "#     def search_in_trie(self, trie, input_text, matched_positions, remaining_chars, reverse):\n",
    "#         match = self.search_part(trie, input_text, matched_positions, remaining_chars, reverse)\n",
    "#         res = match[0]\n",
    "#         if match and res:\n",
    "#             input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "#         return res, input_text\n",
    "\n",
    "#     def search_in_segment(self, segments, trie, category, reverse=False):\n",
    "#         if reverse:\n",
    "#             segments.reverse()\n",
    "\n",
    "#         for seg in segments:\n",
    "#             word = self.autocorrect(seg, trie, category)\n",
    "#             if word == \"\":\n",
    "#                 continue\n",
    "#             segments.remove(seg)\n",
    "#             return word\n",
    "\n",
    "#         if reverse:\n",
    "#             segments.reverse()\n",
    "\n",
    "#         return \"\"\n",
    "\n",
    "#     def search_part(self, trie, input_text, matched_positions, remaining_chars, reversed=False):\n",
    "#         it_range = range(len(input_text))\n",
    "#         if reversed:\n",
    "#             it_range = it_range.__reversed__()\n",
    "#         for i in it_range:\n",
    "#             if i in matched_positions:\n",
    "#                 continue\n",
    "#             match = trie.search_max_length(input_text, i)\n",
    "#             if match:\n",
    "#                 return match\n",
    "#         return \"\", None, None\n",
    "\n",
    "#     def process(self, s: str):\n",
    "#       # Ghi lại chuỗi đầu vào ban đầu\n",
    "#       s_copy = s[:]\n",
    "\n",
    "#       # Tiến hành chia tách và chuẩn hóa chuỗi\n",
    "#       segments = self.segment_text(s)  # Tách các phần của địa chỉ\n",
    "#       input_text = self.normalize_text_but_keep_accent(\",\".join(segments))  # Chuẩn hóa mà không loại bỏ dấu tiếng Việt\n",
    "\n",
    "#       # Khởi tạo kết quả ban đầu cho ward, district, province\n",
    "#       results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "\n",
    "#       # Tìm kiếm trong trie với dấu\n",
    "#       result, remaining_text = self.search_locations_in_trie(self.tries, input_text, results)\n",
    "\n",
    "#       # Nếu không tìm thấy kết quả ở tỉnh/quận/phường, thử tìm kiếm lại không có dấu\n",
    "#       if result[\"province\"] == \"\" and result[\"district\"] == \"\" and result[\"ward\"] == \"\":\n",
    "#           remaining_text = self.normalize_text_and_remove_accent(remaining_text)\n",
    "#           result, remaining_text = self.search_locations_in_trie(self.tries, remaining_text, results)\n",
    "\n",
    "#       # Nếu vẫn chưa tìm thấy, thử tìm kiếm theo các phần của chuỗi (segments)\n",
    "#       if result[\"province\"] == \"\" and result[\"district\"] == \"\" and result[\"ward\"] == \"\":\n",
    "#           segments = self.segment_text(remaining_text, False)\n",
    "#           result, remaining_text = self.search_locations_in_segments(self.tries, segments, results)\n",
    "\n",
    "\n",
    "#       return {\n",
    "#           \"province\": self.tries[\"province\"].get_raw_text(result[\"province\"]),\n",
    "#           \"district\": self.tries[\"district\"].get_raw_text(result[\"district\"]),\n",
    "#           \"ward\": self.tries[\"ward\"].get_raw_text(result[\"ward\"]),\n",
    "#       }\n",
    "\n",
    "#     def debug_address(self, address: str):\n",
    "#         \"\"\"Debugging function to print step-by-step.\"\"\"\n",
    "#         print(\"\\n📌 Input Address:\", address)\n",
    "\n",
    "#         # Step 1: Normalize the address (remove spaces, special cases, etc.)\n",
    "#         preprocessed = self.normalize_text_but_keep_accent(address)\n",
    "#         print(\"🔹 After Preprocessing (with accents):\", preprocessed)\n",
    "\n",
    "#         # Step 2: Segment the address into components (e.g., province, district, ward)\n",
    "#         segments = self.segment_text(address)\n",
    "#         print(\"🔹 Address Segments:\", segments)\n",
    "\n",
    "#         # Step 3: Search for matches in the trie (with accents)\n",
    "#         results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "#         matched_results, remaining_text = self.search_locations_in_trie(self.tries, preprocessed, results)\n",
    "#         print(\"🔹 Extracted Matches (with accents):\", matched_results)\n",
    "#         print(\"🔹 Remaining Text After Accent Search:\", remaining_text)\n",
    "\n",
    "#         # Step 4: If no match found, try to remove accents and search again\n",
    "#         if matched_results[\"province\"] == \"\" and matched_results[\"district\"] == \"\" and matched_results[\"ward\"] == \"\":\n",
    "#             remaining_text_no_accents = self.normalize_text_and_remove_accent(remaining_text)\n",
    "#             matched_results_no_accents, remaining_text = self.search_locations_in_trie(self.tries, remaining_text_no_accents, results)\n",
    "#             print(\"🔹 Extracted Matches (without accents):\", matched_results_no_accents)\n",
    "#             print(\"🔹 Remaining Text After No-Accents Search:\", remaining_text)\n",
    "\n",
    "#         # Step 5: If still no matches, try searching by individual segments\n",
    "#         if matched_results[\"province\"] == \"\" and matched_results[\"district\"] == \"\" and matched_results[\"ward\"] == \"\":\n",
    "#             segments = self.segment_text(remaining_text, False)\n",
    "#             matched_results, remaining_text = self.search_locations_in_segments(self.tries, segments, results)\n",
    "#             print(\"🔹 Matches After Searching by Segments:\", matched_results)\n",
    "#             print(\"🔹 Remaining Text After Searching by Segments:\", remaining_text)\n",
    "\n",
    "#         # Step 6: Unnormalize the final results (convert normalized names back to original form)\n",
    "#         final_results = {\n",
    "#             \"province\": self.tries[\"province\"].get_raw_text(matched_results[\"province\"]),\n",
    "#             \"district\": self.tries[\"district\"].get_raw_text(matched_results[\"district\"]),\n",
    "#             \"ward\": self.tries[\"ward\"].get_raw_text(matched_results[\"ward\"]),\n",
    "#         }\n",
    "\n",
    "#         print(\"✅ Final Address Components:\", final_results)\n",
    "#         return final_results\n",
    "\n",
    "\n",
    "class Solution:\n",
    "\n",
    "    debug=False\n",
    "\n",
    "    def __init__(self):\n",
    "        # list provice, district, ward for private test, do not change for any reason (these file will be provided later with this exact name)\n",
    "\n",
    "        self.province_path = 'list_province.txt'\n",
    "        self.district_path = 'list_district.txt'\n",
    "        self.ward_path = 'list_ward.txt'\n",
    "\n",
    "        self.tries = {}\n",
    "        load_databases({\n",
    "            \"province\": self.province_path,\n",
    "            \"district\": self.district_path,\n",
    "            \"ward\": self.ward_path\n",
    "        }, self.tries)\n",
    "\n",
    "        self.variation_map = variation_map\n",
    "        pass\n",
    "\n",
    "    def process(self, s: str):\n",
    "        # Preprocess\n",
    "        s_copy = s[:]\n",
    "\n",
    "        segments = segment_text(s)\n",
    "        input_text = normalize_text_but_keep_accent(\",\".join(segments))\n",
    "\n",
    "        # Start searching\n",
    "        results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "\n",
    "        # Search with accents\n",
    "        result, remaining_text = search_locations_in_trie(self.tries, input_text, results)\n",
    "\n",
    "        # If the province/district/ward not found, search without accents\n",
    "        # remaining_text = normalize_text_and_remove_accent(remaining_text)\n",
    "        # result, remaining_text = search_locations_in_trie(self.tries, remaining_text, results)\n",
    "\n",
    "        # If the province/district/ward not found, search by segments\n",
    "        segments = segment_text(remaining_text, False)\n",
    "        result, remaining_text = search_locations_in_segments(self.tries, segments, results)\n",
    "\n",
    "        result =  {\n",
    "            \"province\": self.tries[\"province\"].get_raw_text(result[\"province\"]),\n",
    "            \"district\": self.tries[\"district\"].get_raw_text(result[\"district\"]),\n",
    "            \"ward\": self.tries[\"ward\"].get_raw_text(result[\"ward\"]),\n",
    "        }\n",
    "\n",
    "        if self.debug:\n",
    "            print()\n",
    "            print(f\"Original: {s_copy}\")\n",
    "            print(f\"Normalized: {normalize_text_but_keep_accent(s_copy)}\")\n",
    "            print(f\"Result: {result}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "# Điền Hải -> Điên Hải -> Tiên Hải\n",
    "# runner = Solution()\n",
    "# runner.debug = True\n",
    "\n",
    "# runner.process(\"Diên Thạnh,,T Khabnh Hòa\")\n",
    "\n",
    "\n",
    "# Not able to solve yet\n",
    "# runner.process(\" T.P Phan Rang-Tháp lhàm  Ninh Thuận\")\n",
    "# runner.process(\"Điên Hải, Đông Hải, T bạc Liêu\")\n",
    "\n"
   ],
   "metadata": {
    "id": "xtwG3tBDzMLD"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run tests"
   ],
   "metadata": {
    "id": "lLrJr9sLn4J3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CORRECT TESTS\n",
    "groups_province = {}\n",
    "groups_district = {'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'kbang': ['Kbang', 'KBang'], 'quy nhơn': ['Qui Nhơn', 'Quy Nhơn']}\n",
    "groups_ward = {'ái nghĩa': ['ái Nghĩa', 'Ái Nghĩa'], 'ái quốc': ['ái Quốc', 'Ái Quốc'], 'ái thượng': ['ái Thượng', 'Ái Thượng'], 'ái tử': ['ái Tử', 'Ái Tử'], 'ấm hạ': ['ấm Hạ', 'Ấm Hạ'], 'an ấp': ['An ấp', 'An Ấp'], 'ẳng cang': ['ẳng Cang', 'Ẳng Cang'], 'ẳng nưa': ['ẳng Nưa', 'Ẳng Nưa'], 'ẳng tở': ['ẳng Tở', 'Ẳng Tở'], 'an hòa': ['An Hoà', 'An Hòa'], 'ayun': ['Ayun', 'AYun'], 'bắc ái': ['Bắc ái', 'Bắc Ái'], 'bảo ái': ['Bảo ái', 'Bảo Ái'], 'bình hòa': ['Bình Hoà', 'Bình Hòa'], 'châu ổ': ['Châu ổ', 'Châu Ổ'], 'chư á': ['Chư á', 'Chư Á'], 'chư rcăm': ['Chư Rcăm', 'Chư RCăm'], 'cộng hòa': ['Cộng Hoà', 'Cộng Hòa'], 'cò nòi': ['Cò  Nòi', 'Cò Nòi'], 'đại ân 2': ['Đại Ân  2', 'Đại Ân 2'], 'đak ơ': ['Đak ơ', 'Đak Ơ'], \"đạ m'ri\": [\"Đạ M'ri\", \"Đạ M'Ri\"], 'đông hòa': ['Đông Hoà', 'Đông Hòa'], 'đồng ích': ['Đồng ích', 'Đồng Ích'], 'hải châu i': ['Hải Châu  I', 'Hải Châu I'], 'hải hòa': ['Hải Hoà', 'Hải Hòa'], 'hành tín đông': ['Hành Tín  Đông', 'Hành Tín Đông'], 'hiệp hòa': ['Hiệp Hoà', 'Hiệp Hòa'], 'hòa bắc': ['Hoà Bắc', 'Hòa Bắc'], 'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'hòa châu': ['Hoà Châu', 'Hòa Châu'], 'hòa hải': ['Hoà Hải', 'Hòa Hải'], 'hòa hiệp trung': ['Hoà Hiệp Trung', 'Hòa Hiệp Trung'], 'hòa liên': ['Hoà Liên', 'Hòa Liên'], 'hòa lộc': ['Hoà Lộc', 'Hòa Lộc'], 'hòa lợi': ['Hoà Lợi', 'Hòa Lợi'], 'hòa long': ['Hoà Long', 'Hòa Long'], 'hòa mạc': ['Hoà Mạc', 'Hòa Mạc'], 'hòa minh': ['Hoà Minh', 'Hòa Minh'], 'hòa mỹ': ['Hoà Mỹ', 'Hòa Mỹ'], 'hòa phát': ['Hoà Phát', 'Hòa Phát'], 'hòa phong': ['Hoà Phong', 'Hòa Phong'], 'hòa phú': ['Hoà Phú', 'Hòa Phú'], 'hòa phước': ['Hoà Phước', 'Hòa Phước'], 'hòa sơn': ['Hoà Sơn', 'Hòa Sơn'], 'hòa tân': ['Hoà Tân', 'Hòa Tân'], 'hòa thuận': ['Hoà Thuận', 'Hòa Thuận'], 'hòa tiến': ['Hoà Tiến', 'Hòa Tiến'], 'hòa trạch': ['Hoà Trạch', 'Hòa Trạch'], 'hòa vinh': ['Hoà Vinh', 'Hòa Vinh'], 'hương hòa': ['Hương Hoà', 'Hương Hòa'], 'ích hậu': ['ích Hậu', 'Ích Hậu'], 'ít ong': ['ít Ong', 'Ít Ong'], 'khánh hòa': ['Khánh Hoà', 'Khánh Hòa'], 'krông á': ['Krông Á', 'KRông á'], 'lộc hòa': ['Lộc Hoà', 'Lộc Hòa'], 'minh hòa': ['Minh Hoà', 'Minh Hòa'], 'mường ải': ['Mường ải', 'Mường Ải'], 'mường ẳng': ['Mường ẳng', 'Mường Ẳng'], 'nậm ét': ['Nậm ét', 'Nậm Ét'], 'nam hòa': ['Nam Hoà', 'Nam Hòa'], 'na ư': ['Na ư', 'Na Ư'], 'ngã sáu': ['Ngã sáu', 'Ngã Sáu'], 'nghi hòa': ['Nghi Hoà', 'Nghi Hòa'], 'nguyễn úy': ['Nguyễn Uý', 'Nguyễn úy', 'Nguyễn Úy'], 'nhân hòa': ['Nhân Hoà', 'Nhân Hòa'], 'nhơn hòa': ['Nhơn Hoà', 'Nhơn Hòa'], 'nhơn nghĩa a': ['Nhơn nghĩa A', 'Nhơn Nghĩa A'], 'phúc ứng': ['Phúc ứng', 'Phúc Ứng'], 'phước hòa': ['Phước Hoà', 'Phước Hòa'], 'sơn hóa': ['Sơn Hoá', 'Sơn Hóa'], 'tạ an khương đông': ['Tạ An Khương  Đông', 'Tạ An Khương Đông'], 'tạ an khương nam': ['Tạ An Khương  Nam', 'Tạ An Khương Nam'], 'tăng hòa': ['Tăng Hoà', 'Tăng Hòa'], 'tân hòa': ['Tân Hoà', 'Tân Hòa'], 'tân hòa thành': ['Tân Hòa  Thành', 'Tân Hòa Thành'], 'tân khánh trung': ['Tân  Khánh Trung', 'Tân Khánh Trung'], 'tân lợi': ['Tân lợi', 'Tân Lợi'], 'thái hòa': ['Thái Hoà', 'Thái Hòa'], 'thiết ống': ['Thiết ống', 'Thiết Ống'], 'thuận hòa': ['Thuận Hoà', 'Thuận Hòa'], 'thượng ấm': ['Thượng ấm', 'Thượng Ấm'], 'thụy hương': ['Thuỵ Hương', 'Thụy Hương'], 'thủy xuân': ['Thuỷ Xuân', 'Thủy Xuân'], 'tịnh ấn đông': ['Tịnh ấn Đông', 'Tịnh Ấn Đông'], 'tịnh ấn tây': ['Tịnh ấn Tây', 'Tịnh Ấn Tây'], 'triệu ái': ['Triệu ái', 'Triệu Ái'], 'triệu ẩu': ['Triệu ẩu', 'Triệu Ẩu'], 'trung hòa': ['Trung Hoà', 'Trung Hòa'], 'trung ý': ['Trung ý', 'Trung Ý'], 'tùng ảnh': ['Tùng ảnh', 'Tùng Ảnh'], 'úc kỳ': ['úc Kỳ', 'Úc Kỳ'], 'ứng hòe': ['ứng Hoè', 'Ứng Hoè'], 'vĩnh hòa': ['Vĩnh Hoà', 'Vĩnh Hòa'], 'vũ hòa': ['Vũ Hoà', 'Vũ Hòa'], 'xuân ái': ['Xuân ái', 'Xuân Ái'], 'xuân áng': ['Xuân áng', 'Xuân Áng'], 'xuân hòa': ['Xuân Hoà', 'Xuân Hòa'], 'xuất hóa': ['Xuất Hoá', 'Xuất Hóa'], 'ỷ la': ['ỷ La', 'Ỷ La']}\n",
    "groups_ward.update({1: ['1', '01'], 2: ['2', '02'], 3: ['3', '03'], 4: ['4', '04'], 5: ['5', '05'], 6: ['6', '06'], 7: ['7', '07'], 8: ['8', '08'], 9: ['9', '09']})\n",
    "def to_same(groups):\n",
    "    same = {ele: k for k, v in groups.items() for ele in v}\n",
    "    return same\n",
    "same_province = to_same(groups_province)\n",
    "same_district = to_same(groups_district)\n",
    "same_ward = to_same(groups_ward)\n",
    "def normalize(text, same_dict):\n",
    "    return same_dict.get(text, text)"
   ],
   "metadata": {
    "id": "fyY3ymB8n6R7"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# NOTE: DO NOT change this cell\n",
    "# This cell is for downloading private test\n",
    "!rm -rf test.json\n",
    "# this link is public test\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB/view?usp=sharing -O test.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggOye_EFowUu",
    "outputId": "be0efa80-82c7-4c88-f9d5-72f657252697"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB\n",
      "To: /content/test.json\n",
      "\r  0% 0.00/79.4k [00:00<?, ?B/s]\r100% 79.4k/79.4k [00:00<00:00, 61.8MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "debug = True\n",
    "\n",
    "TEAM_NAME = 'GROUP4'  # This should be your team name\n",
    "EXCEL_FILE = f'{TEAM_NAME}.xlsx'\n",
    "\n",
    "import json\n",
    "import time\n",
    "with open('test.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "summary_only = True\n",
    "df = []\n",
    "solution = Solution()\n",
    "timer = []\n",
    "correct = 0\n",
    "for test_idx, data_point in enumerate(data):\n",
    "    address = data_point[\"text\"]\n",
    "\n",
    "    ok = 0\n",
    "    try:\n",
    "        answer = data_point[\"result\"]\n",
    "        answer[\"province_normalized\"] = normalize(answer[\"province\"], same_province)\n",
    "        answer[\"district_normalized\"] = normalize(answer[\"district\"], same_district)\n",
    "        answer[\"ward_normalized\"] = normalize(answer[\"ward\"], same_ward)\n",
    "\n",
    "        start = time.perf_counter_ns()\n",
    "        result = solution.process(address)\n",
    "        finish = time.perf_counter_ns()\n",
    "        timer.append(finish - start)\n",
    "        result[\"province_normalized\"] = normalize(result[\"province\"], same_province)\n",
    "        result[\"district_normalized\"] = normalize(result[\"district\"], same_district)\n",
    "        result[\"ward_normalized\"] = normalize(result[\"ward\"], same_ward)\n",
    "\n",
    "        province_correct = int(answer[\"province_normalized\"] == result[\"province_normalized\"])\n",
    "        district_correct = int(answer[\"district_normalized\"] == result[\"district_normalized\"])\n",
    "        ward_correct = int(answer[\"ward_normalized\"] == result[\"ward_normalized\"])\n",
    "        ok = province_correct + district_correct + ward_correct\n",
    "\n",
    "        df.append([\n",
    "            test_idx,\n",
    "            address,\n",
    "            answer[\"province\"],\n",
    "            result[\"province\"],\n",
    "            answer[\"province_normalized\"],\n",
    "            result[\"province_normalized\"],\n",
    "            province_correct,\n",
    "            answer[\"district\"],\n",
    "            result[\"district\"],\n",
    "            answer[\"district_normalized\"],\n",
    "            result[\"district_normalized\"],\n",
    "            district_correct,\n",
    "            answer[\"ward\"],\n",
    "            result[\"ward\"],\n",
    "            answer[\"ward_normalized\"],\n",
    "            result[\"ward_normalized\"],\n",
    "            ward_correct,\n",
    "            ok,\n",
    "            timer[-1] / 1_000_000_000,\n",
    "        ])\n",
    "        if debug and ok < 3:\n",
    "          print()\n",
    "          print(\"Original: \" + address)\n",
    "          if province_correct == 0:\n",
    "              print(f\"Province -> Result: '{result['province']}', Answer: '{answer['province']}'\")\n",
    "          if district_correct == 0:\n",
    "              print(f\"District -> Result: '{result['district']}', Answer: '{answer['district']}'\")\n",
    "          if ward_correct == 0:\n",
    "              print(f\"Ward -> Result: '{result['ward']}', Answer: '{answer['ward']}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{answer = }\")\n",
    "        print(f\"{result = }\")\n",
    "        df.append([\n",
    "            test_idx,\n",
    "            address,\n",
    "            answer[\"province\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"province_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            answer[\"district\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"district_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            answer[\"ward\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"ward_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])\n",
    "        # any failure count as a zero correct\n",
    "        pass\n",
    "    correct += ok\n",
    "\n",
    "\n",
    "    if not summary_only:\n",
    "        # responsive stuff\n",
    "        print(f\"Test {test_idx:5d}/{len(data):5d}\")\n",
    "        print(f\"Correct: {ok}/3\")\n",
    "        print(f\"Time Executed: {timer[-1] / 1_000_000_000:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"-\"*30)\n",
    "total = len(data) * 3\n",
    "score_scale_10 = round(correct / total * 10, 2)\n",
    "if len(timer) == 0:\n",
    "    timer = [0]\n",
    "max_time_sec = round(max(timer) / 1_000_000_000, 4)\n",
    "avg_time_sec = round((sum(timer) / len(timer)) / 1_000_000_000, 4)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    [[correct, total, score_scale_10, max_time_sec, avg_time_sec]],\n",
    "    columns=['correct', 'total', 'score / 10', 'max_time_sec', 'avg_time_sec',],\n",
    ")\n",
    "\n",
    "columns = [\n",
    "    'ID',\n",
    "    'text',\n",
    "    'province',\n",
    "    'province_student',\n",
    "    'province_normalized',\n",
    "    'province_student_normalized',\n",
    "    'province_correct',\n",
    "    'district',\n",
    "    'district_student',\n",
    "    'district_normalized',\n",
    "    'district_student_normalized',\n",
    "    'district_correct',\n",
    "    'ward',\n",
    "    'ward_student',\n",
    "    'ward_normalized',\n",
    "    'ward_student_normalized',\n",
    "    'ward_correct',\n",
    "    'total_correct',\n",
    "    'time_sec',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = columns\n",
    "\n",
    "print(df2)\n",
    "\n",
    "!pip install xlsxwriter\n",
    "\n",
    "if not debug:\n",
    "    print(f'{TEAM_NAME = }')\n",
    "    print(f'{EXCEL_FILE = }')\n",
    "    writer = pd.ExcelWriter(EXCEL_FILE, engine='xlsxwriter')\n",
    "    df2.to_excel(writer, index=False, sheet_name='summary')\n",
    "    df.to_excel(writer, index=False, sheet_name='details')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# Thái Hòa, Hợp Đồng Chương Mỹ, Hà Nội\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZZNgQSwn-LK",
    "outputId": "c8873d2e-5620-4276-961b-9a108f1e3bc4"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Original: 357/28,Ng-T- Thuật,P1,Q3,TP.HồChíMinh.\n",
      "Ward -> Result: '12', Answer: ''\n",
      "\n",
      "Original: 284DBis Ng Văn Giáo, P3, Mỹ Tho, T.Giang.\n",
      "Province -> Result: 'An Giang', Answer: 'Tiền Giang'\n",
      "\n",
      "Original: Tiểu khu 3, thị trấn Ba Hàng, huyện Phổ Yên, tỉnh Thái Nguyên.\n",
      "Province -> Result: 'Thái Nguyên', Answer: ''\n",
      "District -> Result: 'Phổ Yên', Answer: ''\n",
      "\n",
      "Original: 154/4/81 Nguyễn - Phúc Chu, P15, TB, TP. Hồ Chí Minh\n",
      "Province -> Result: 'Hồ Chí Minh', Answer: ''\n",
      "Ward -> Result: 'Chợ Chu', Answer: ''\n",
      "\n",
      "Original: Tổ 8, Ấp An Bình Minh Hòa, Châu Thành, Kiên Giang\n",
      "Ward -> Result: 'Bình Minh', Answer: 'Minh Hòa'\n",
      "\n",
      "Original: Phố Đức Sơn, TT Bút Sơn, Hoằng Hoá, Thanh Hoá.\n",
      "Ward -> Result: 'Đức Sơn', Answer: 'Bút Sơn'\n",
      "\n",
      "Original: 285 B/1A Bình Gĩa Phường 8,Vũng Tàu,Bà Rịa - Vũng Tàu\n",
      "Province -> Result: '', Answer: 'Bà Rịa - Vũng Tàu'\n",
      "\n",
      "Original: KP Phước Trung Thị trấn Đất Đỏ, Đất Đỏ, Bà Rịa - Vũng Tàu\n",
      "Province -> Result: '', Answer: 'Bà Rịa - Vũng Tàu'\n",
      "\n",
      "Original: 55/30 Nguyễn - Thượng Hiền,P5,BT, TP. Hồ Chí Minh\n",
      "District -> Result: '', Answer: 'Bình Thạnh'\n",
      "\n",
      "Original: Thái Hòa, Hợp Đồng Chương Mỹ, Hà Nội\n",
      "Ward -> Result: 'Thái Hòa', Answer: 'Hợp Đồng'\n",
      "\n",
      "Original: Khu phố Nam Tân, TT Thuận Nam, Hàm Thuận Bắc, Bình Thuận.\n",
      "Province -> Result: 'Bình Thuận', Answer: ''\n",
      "Ward -> Result: 'Nậm Tin', Answer: ''\n",
      "\n",
      "Original: 2 dãy C ngõ 16 Ngô Quyền, tổ 14, Quang Trung, Hà Đông, Hà Nội\n",
      "Ward -> Result: 'Ngô Quyền', Answer: 'Quang Trung'\n",
      "\n",
      "Original: Long Bình 1, An Hải, Ninh Phước, Ninh Thuận\n",
      "Ward -> Result: 'Long Bình', Answer: 'An Hải'\n",
      "\n",
      "Original: - Khu B Chu Hoà, Việt HhiPhú Thọ\n",
      "Province -> Result: 'Phú Thọ', Answer: ''\n",
      "\n",
      "Original: A:12A.21BlockA C/c BCA,P.AnKhánh,TP.Thủ Đức, TP. HCM\n",
      "Ward -> Result: 'Xuân Khanh', Answer: ''\n",
      "\n",
      "Original: thôn 2 Suối Rao, Châu Đức, Bà Rịa - Vũng Tàu\n",
      "Province -> Result: '', Answer: 'Bà Rịa - Vũng Tàu'\n",
      "\n",
      "Original: Long Bình, An Hải, Ninh Phước, Ninh Thuận\n",
      "Ward -> Result: 'Long Bình', Answer: 'An Hải'\n",
      "\n",
      "Original: Số 1410 Đường 30/4, Phường 12, Thành phố Vũng Tàu, Bà Rịa - Vũng Tàu.\n",
      "Province -> Result: '', Answer: 'Bà Rịa - Vũng Tàu'\n",
      "\n",
      "Original: F2B PhanVănTr P.5,GV,TP.Hồ Chí Minh\n",
      "Ward -> Result: '5', Answer: ''\n",
      "\n",
      "Original: X Cam Bình, Thành FhốCam Rah,  Khánh Hòa\n",
      "District -> Result: 'Thanh Hà', Answer: 'Cam Ranh'\n",
      "\n",
      "Original:  Minh Tân,h.Lưng Tai,\n",
      "District -> Result: '', Answer: 'Lương Tài'\n",
      "\n",
      "Original:  Huyện Hiêp Hòa Bắc Giang\n",
      "District -> Result: '', Answer: 'Hiệp Hòa'\n",
      "Ward -> Result: 'Minh Hòa', Answer: ''\n",
      "\n",
      "Original:   Đông Giang T Quảyg Nm\n",
      "District -> Result: '', Answer: 'Đông Giang'\n",
      "Ward -> Result: 'Đông', Answer: ''\n",
      "\n",
      "Original: XãV Ninh, Kiên Xương,\n",
      "Province -> Result: 'Kiên Giang', Answer: ''\n",
      "District -> Result: '', Answer: 'Kiến Xương'\n",
      "\n",
      "Original: Phưng Khâm Thiên Quận Đ.Đa T.Phố HàNội\n",
      "District -> Result: 'Hưng Hà', Answer: 'Đống Đa'\n",
      "\n",
      "Original: XVnh Hung, , T.Thanh Hóa\n",
      "Ward -> Result: 'Vĩnh Hưng', Answer: 'Vĩnh Hùng'\n",
      "\n",
      "Original: Phưong Thọ Son,T.Phố Việt Trì, Phú Thọ\n",
      "Ward -> Result: '', Answer: 'Thọ Sơn'\n",
      "\n",
      "Original: X.Chê La,,T.Hà GianZ\n",
      "Ward -> Result: '', Answer: 'Chế Là'\n",
      "\n",
      "Original: Hôno Sỹ  Tỉnh Cao Bằng\n",
      "Ward -> Result: '', Answer: 'Hồng Sỹ'\n",
      "\n",
      "Original: Fmễ Trì, Nam Từ Liêm, Thành phố HN\n",
      "Province -> Result: '', Answer: 'Hà Nội'\n",
      "\n",
      "Original:  Long Thắng, Huyn Lai ung, \n",
      "District -> Result: '', Answer: 'Lai Vung'\n",
      "\n",
      "Original: X. Tân An, HuyệnThnh Hà, \n",
      "Province -> Result: 'Thanh Hóa', Answer: ''\n",
      "District -> Result: '', Answer: 'Thanh Hà'\n",
      "\n",
      "Original: Xã Nâm Ntừ  Nậm Pồ \n",
      "Ward -> Result: '', Answer: 'Nậm Nhừ'\n",
      "\n",
      "Original: ,HPhú Bình, thái Nguyên\n",
      "District -> Result: '', Answer: 'Phú Bình'\n",
      "Ward -> Result: 'Phú Bình', Answer: ''\n",
      "\n",
      "Original:  Khả cửu, H. Thanh Srn, \n",
      "Province -> Result: 'Thanh Hóa', Answer: ''\n",
      "District -> Result: '', Answer: 'Thanh Sơn'\n",
      "\n",
      "Original: , Đông Hòa,Tỉnh Phú yn\n",
      "District -> Result: 'Phú Hoà', Answer: 'Đông Hòa'\n",
      "Ward -> Result: 'Đông', Answer: ''\n",
      "\n",
      "Original: Xa Sơn Thy H. Le Thủy Tỉnh Quảng Bình\n",
      "Ward -> Result: '', Answer: 'Sơn Thủy'\n",
      "\n",
      "Original:  Đông Htòa Phu Yên\n",
      "Province -> Result: '', Answer: 'Phú Yên'\n",
      "District -> Result: '', Answer: 'Đông Hòa'\n",
      "Ward -> Result: 'Đông', Answer: ''\n",
      "\n",
      "Original: , ThăngFBìh,T. Quảng Nam\n",
      "District -> Result: '', Answer: 'Thăng Bình'\n",
      "\n",
      "Original: X.H Mỗ, Đan PhTượng,\n",
      "Ward -> Result: '', Answer: 'Hạ Mỗ'\n",
      "\n",
      "Original: X. Lộc Thah,T0P Bảo lộc,TLâm Đông\n",
      "Province -> Result: '', Answer: 'Lâm Đồng'\n",
      "Ward -> Result: 'Đông', Answer: 'Lộc Thanh'\n",
      "\n",
      "Original: xã Vạn Kim, HuyệnMỹ Đức, T.P HNội\n",
      "Province -> Result: '', Answer: 'Hà Nội'\n",
      "\n",
      "Original: X Phu Mãn   HN\n",
      "Province -> Result: '', Answer: 'Hà Nội'\n",
      "\n",
      "Original: Tích Sơn T.Phốvinh Yên t vĩnh phúc\n",
      "District -> Result: '', Answer: 'Vĩnh Yên'\n",
      "\n",
      "Original: X.Hòa Trị Huzyen hú Hoa TPhú Yên\n",
      "District -> Result: '', Answer: 'Phú Hoà'\n",
      "\n",
      "Original: ,H.Trảng Bom,Đồng Nai\n",
      "District -> Result: '', Answer: 'Trảng Bom'\n",
      "Ward -> Result: 'Trảng Bom', Answer: ''\n",
      "\n",
      "Original: X. Sơn Hv HSơn Hoa Tỉnh Phú Yên\n",
      "District -> Result: '', Answer: 'Sơn Hòa'\n",
      "Ward -> Result: '', Answer: 'Sơn Hội'\n",
      "\n",
      "Original: Xã Tân Kiên Hbình Chánh Thành phôHôChíMinh\n",
      "Province -> Result: '', Answer: 'Hồ Chí Minh'\n",
      "\n",
      "Original: X. Tam Đồng,HuyệnMê Linh,T.Phw HàNoi\n",
      "Province -> Result: '', Answer: 'Hà Nội'\n",
      "\n",
      "Original:  eăn Tiến, Yên Lạc, vĩnh P0húc\n",
      "District -> Result: '', Answer: 'Yên Lạc'\n",
      "Ward -> Result: 'Yên Lạc', Answer: 'Văn Tiến'\n",
      "\n",
      "Original: Diên Thạnh,,T Khabnh Hòa\n",
      "Province -> Result: 'Thanh Hóa', Answer: 'Khánh Hòa'\n",
      "\n",
      "Original:  Quận 1 T.P H.C.Minh\n",
      "Ward -> Result: 'Vũ Ninh', Answer: ''\n",
      "\n",
      "Original: ,H.Tuy An,Tinh Phú yên\n",
      "Ward -> Result: 'Tân Lĩnh', Answer: ''\n",
      "\n",
      "Original:  T.P Phan Rang-Tháp lhàm  Ninh Thuận\n",
      "District -> Result: '', Answer: 'Phan Rang-Tháp Chàm'\n",
      "\n",
      "Original:  Thái Ha, HBa Vì, T.pHNội\n",
      "Province -> Result: '', Answer: 'Hà Nội'\n",
      "\n",
      "Original: , Nam Đông,T. T.T.H\n",
      "Province -> Result: 'Hà Nam', Answer: 'Thừa Thiên Huế'\n",
      "District -> Result: '', Answer: 'Nam Đông'\n",
      "Ward -> Result: 'Đông', Answer: ''\n",
      "\n",
      "Original:  Cam Nhân HuyệnYên Bình TYên Bái\n",
      "District -> Result: '', Answer: 'Yên Bình'\n",
      "Ward -> Result: 'Yên Bình', Answer: 'Cảm Nhân'\n",
      "\n",
      "Original:  Điên Hải, Đông Hải, T bạc Liêu\n",
      "District -> Result: 'Tiền Hải', Answer: 'Đông Hải'\n",
      "Ward -> Result: 'Đông', Answer: 'Điền Hải'\n",
      "\n",
      "Original: Đông thanh AnE Minh T kien Giang\n",
      "District -> Result: '', Answer: 'An Minh'\n",
      "Ward -> Result: 'Đông', Answer: 'Đông Thạnh'\n",
      "\n",
      "Original: P Thủy Châu,T.X. hươngThủy,Thừa.t.Huế\n",
      "Province -> Result: '', Answer: 'Thừa Thiên Huế'\n",
      "------------------------------\n",
      "   correct  total  score / 10  max_time_sec  avg_time_sec\n",
      "0     1269   1350         9.4        0.0311        0.0023\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m165.1/165.1 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.2\n"
     ]
    }
   ]
  }
 ]
}
