{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# If you want to use your own database, download it here\n",
        "# !gdown ...\n",
        "!rm -rf list_province.txt\n",
        "!rm -rf list_district.txt\n",
        "!rm -rf list_ward.txt\n",
        "\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1FC5Kb1iL_ElLboQ_yuJt-RyujZWVBe5N/view?usp=sharing -O list_province.txt\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1mLEuPqn_01ffMGnDy2fntHhpWSNyYHSW/view?usp=sharing -O list_district.txt\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1LmCBNSqf2fY4lcphAFgQlMR_GkCwSLkk/view?usp=sharing -O list_ward.txt"
      ],
      "metadata": {
        "id": "i20WfB6lqiUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# Add more to your needs\n",
        "# you must place ALL pip install here\n",
        "!pip install editdistance"
      ],
      "metadata": {
        "id": "J8znFuZTzwoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# import your library here\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import unicodedata\n",
        "import json\n",
        "from collections import Counter\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from queue import Queue\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import editdistance\n",
        "from gettext import textdomain\n",
        "from numpy import character"
      ],
      "metadata": {
        "id": "AodaIxYa32hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utils\n",
        "import re\n",
        "from gettext import textdomain\n",
        "\n",
        "import unicodedata\n",
        "from numpy import character\n",
        "# Danh sách các prefix cần loại bỏ\n",
        "\n",
        "def remove_prefix(name):\n",
        "    \"\"\"Loại bỏ các prefix trong danh sách ALL_PREFIXES khỏi tên.\"\"\"\n",
        "    name = name.strip()\n",
        "    # Sắp xếp prefix từ dài đến ngắn để tránh trường hợp prefix ngắn bị ưu tiên\n",
        "    sorted_prefixes = sorted(ALL_PREFIXES, key=len, reverse=True)\n",
        "\n",
        "    for prefix in sorted_prefixes:\n",
        "        if name.startswith(prefix):\n",
        "            name = name[len(prefix):].strip()\n",
        "\n",
        "    return name\n",
        "\n",
        "\n",
        "SAFE_CASES = [\n",
        "                \"TP.\", \"TP\",\"Thành phố\", \"ThànhPhố\", \"T.P\", # \"T.P\", \"F.\", \"TP \", \" TP\",\n",
        "               \"Tỉnh\", \" T \", #\"Tỉn\",  \",T \", \"T.\",\n",
        "               #\"Quận\", \"Q.\", \" Q \", \",Q \", -> quận 5 sau khi remove sẽ thành 5 và không tìm ra\n",
        "               \"Huyện\", \"hyện\", #\"H.\", \" H \", \",H \",\n",
        "               # \"Phường\", \"F\",  \"P.\", \" P \", \",P \",\n",
        "               \"Thị xã\", \"ThịXã\", \"Xã\", # \"X.\", \" X \", \"X \", \",X \",\n",
        "                \"Thị trấn\", \"ThịTrấn\", #\"T.T\",\n",
        "                \"khu phố\", \"KP\", \"KhuPhố\", \"Khu pho\", \"KhuPho\", # -> KP5 bị nhầm thành P5\n",
        "               ]\n",
        "\n",
        "ALL_PREFIXES = [\"TP.\", \"T.P\", \"F.\", \"Thành phố\", \"ThànhPhố\", \"TP \", \" TP\",\n",
        "               \"Tỉnh\", \"Tỉn\",\"T.\", \" T \",  #\",T \",\n",
        "               \"Quận\", \"Q.\", \" Q \", \",Q \",\n",
        "               \"Huyện\", \"hyện\", \"H.\", \" H \",  #\",H \",\n",
        "                #\"KP\", # KP. contains P.\n",
        "               \"khu phố\",  \"KhuPhố\", \"Khu pho\", \"KhuPho\",\n",
        "               \"Phường\", \"P.\", \"F\", \" P \",  #\",P \",\n",
        "               \"X.\", \"Thị xã\", \"ThịXã\", \"Xã\",  #\" X \", \"X \", \",X \",\n",
        "                \"Thị trấn\", \"ThịTrấn\", \"T.T\",\n",
        "               \"-\"\n",
        "                ]\n",
        "\n",
        "vietnamese_dict = {\n",
        "    \"a\": [\"à\", \"á\", \"ạ\", \"ả\", \"ã\", \"â\", \"ầ\", \"ấ\", \"ậ\", \"ẩ\", \"ẫ\", \"ă\", \"ằ\", \"ắ\", \"ặ\", \"ẳ\", \"ẵ\"],\n",
        "    \"d\": [\"đ\"],\n",
        "    \"e\": [\"è\", \"é\", \"ẹ\", \"ẻ\", \"ẽ\", \"ê\", \"ề\", \"ế\", \"ệ\", \"ể\", \"ễ\"],\n",
        "    \"i\": [\"ì\", \"í\", \"ị\", \"ỉ\", \"ĩ\"],\n",
        "    \"o\": [\"ò\", \"ó\", \"ọ\", \"ỏ\", \"õ\", \"ô\", \"ồ\", \"ố\", \"ộ\", \"ổ\", \"ỗ\", \"ơ\", \"ờ\", \"ớ\", \"ợ\", \"ở\", \"ỡ\"],\n",
        "    \"u\": [\"ù\", \"ú\", \"ụ\", \"ủ\", \"ũ\", \"ư\", \"ừ\", \"ứ\", \"ự\", \"ử\", \"ữ\"],\n",
        "    \"y\": [\"ỳ\", \"ý\", \"ỵ\", \"ỷ\", \"ỹ\"],\n",
        "\n",
        "}\n",
        "\n",
        "reversed_vietnamese_dict = {}\n",
        "\n",
        "wrong_accents = {\n",
        "    \"oà\": \"òa\", \"oá\": \"óa\", \"oạ\": \"ọa\", \"oã\": \"õa\", \"oả\": \"ỏa\",\n",
        "    \"qui\": \"quy\",\n",
        "}\n",
        "\n",
        "def common_normalize(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    # text = text.replace(\",\", \"\")  # replace for T,â,n,B,ì,n,h Dĩ An Bình Dương\n",
        "    # text = text.replace(\".\", \"\")\n",
        "    for case in SAFE_CASES:\n",
        "        text = re.sub(re.escape(case), ',', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\s{2,}\", \",\", text)  # Remove spaces\n",
        "    text = text.replace(\" \", \"\")\n",
        "    return text\n",
        "\n",
        "def normalize_text_and_remove_accent(text: str) -> str:\n",
        "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
        "    text = common_normalize(text)\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = text.replace(\"đ\", \"d\")\n",
        "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
        "    # text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
        "    return text\n",
        "\n",
        "def normalize_text_but_keep_vietnamese_alphabet(text: str) -> str:\n",
        "    text = common_normalize(text)\n",
        "    for base_char, variations in vietnamese_dict.items():\n",
        "        for char in variations:\n",
        "            reversed_vietnamese_dict[char] = base_char\n",
        "\n",
        "    result = []\n",
        "    for char in text:\n",
        "        if char in reversed_vietnamese_dict:\n",
        "            result.append(reversed_vietnamese_dict[char])\n",
        "        else:\n",
        "            result.append(char)\n",
        "\n",
        "    text = \"\".join(result)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
        "    text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
        "    return text\n",
        "\n",
        "def normalize_text_but_keep_accent(text: str) -> str:\n",
        "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
        "    text = common_normalize(text)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    for wrong, correct in wrong_accents.items():\n",
        "        text = text.replace(wrong, correct)\n",
        "\n",
        "    return text\n",
        "\n",
        "def segment_text(s, safe=True):\n",
        "    text = s[:]\n",
        "\n",
        "    prefixes = SAFE_CASES if safe else ALL_PREFIXES\n",
        "\n",
        "    for p in prefixes:\n",
        "        text = re.sub(re.escape(p), ',', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Xử lý dấu \"-\" trong tên (ví dụ: \"Ng-T-\" -> \"Ng T \")\n",
        "    text = re.sub(r'[.]', ' ', text)\n",
        "\n",
        "    # Tách cụm địa chỉ\n",
        "    segments = [seg.strip() for seg in text.split(',') if seg.strip()]\n",
        "    #\n",
        "    # print()\n",
        "    # print(f\"'{s}'  -->  {segments}\")\n",
        "    return segments\n",
        "\n",
        "\n",
        "def two_grams(segments):\n",
        "    if not segments:\n",
        "        return []\n",
        "\n",
        "    result = []\n",
        "    n = len(segments)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            result.append(segments[i] + segments[j])\n",
        "    return result\n",
        "\n",
        "def best_candidate_by_distance(search_key, candidates):\n",
        "    best_distance = float('inf')\n",
        "    best_candidate = None\n",
        "    best_reference = None  # Giờ đây chỉ lưu một chuỗi hoặc None\n",
        "\n",
        "    for candidate, reference in candidates:\n",
        "        d = levenshtein_distance(candidate, search_key)\n",
        "        if d < best_distance:\n",
        "            best_distance = d\n",
        "            best_candidate = candidate\n",
        "            best_reference = reference\n",
        "        elif d == best_distance:\n",
        "            # Nếu có nhiều candidate với cùng khoảng cách, bạn có thể quyết định cách chọn:\n",
        "            # Ví dụ, nếu best_reference khác candidate hiện tại, bạn có thể lưu cả hai trong một tập hợp\n",
        "            if best_reference is None:\n",
        "                best_reference = reference\n",
        "            elif best_reference != reference:\n",
        "                # Nếu cần lưu tất cả, bạn có thể chuyển thành tập hợp\n",
        "                if isinstance(best_reference, set):\n",
        "                    best_reference.add(reference)\n",
        "                else:\n",
        "                    best_reference = {best_reference, reference}\n",
        "    return best_candidate, best_reference, best_distance\n",
        "\n",
        "\n",
        "def levenshtein_distance(s, t):\n",
        "    m, n = len(s), len(t)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s[i - 1] == t[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,  # deletion\n",
        "                dp[i][j - 1] + 1,  # insertion\n",
        "                dp[i - 1][j - 1] + cost  # substitution\n",
        "            )\n",
        "    return dp[m][n]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6JlHZVxZeDaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Index Analyzer\n",
        "from queue import Queue\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "\n",
        "locality_map = {}\n",
        "current_id = 0\n",
        "\n",
        "SPECIAL_CASES = [\"xã\", \"x.\", \"huyện\", \"tỉnh\", \"t.\",\n",
        "                 \"tp\",\"thành phố\", \"thànhphố\"]\n",
        "\n",
        "# Prefixes for wards and districts to expand possible matches\n",
        "DIGIT_CASES = {\n",
        "    \"ward\": [\"p\", \"phường\"],\n",
        "    \"district\": [\"q\", \"quận\"],\n",
        "}\n",
        "\n",
        "# Dictionary to store generated variations for tracing back\n",
        "variation_map: Dict[str, dict] = {}\n",
        "\n",
        "province_short_form = {\n",
        "    \"hồchíminh\":\"hcm\",\n",
        "    \"bàrịavũngtàu\":\"brvt\",\n",
        "    \"thừathiênhuế\":\"tth\",\n",
        "    \"tiềngiang\":\"tgiang\",\n",
        "    \"thừathiênhuế\":\"thừathuế\",\n",
        "    \"hànội\":\"hn\"\n",
        "}\n",
        "\n",
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False\n",
        "        self.original_string: Optional[str] = None\n",
        "\n",
        "\n",
        "class Trie:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "        self.all_words = set()\n",
        "        self.original_names: Dict[str, str] = {}\n",
        "\n",
        "    def insert(self, normalized_word: str):\n",
        "        \"\"\"Insert a normalized word into the trie with a reference to the original.\"\"\"\n",
        "        node = self.root\n",
        "        for i, char in enumerate(normalized_word):\n",
        "\n",
        "            if char not in node.children:\n",
        "                node.children[char] = TrieNode()\n",
        "\n",
        "            node = node.children[char]\n",
        "\n",
        "        node.is_end_of_word = True\n",
        "        node.original_string = normalized_word\n",
        "\n",
        "        self.all_words.add(normalized_word)\n",
        "\n",
        "    def search(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
        "        \"\"\"Finds the first valid word from the given start index.\"\"\"\n",
        "        node = self.root\n",
        "        for i in range(start, len(text)):\n",
        "            char = text[i]\n",
        "            if char not in node.children:\n",
        "                break\n",
        "            node = node.children[char]\n",
        "            if node.is_end_of_word:\n",
        "                return (node.original_string, start, i + 1)\n",
        "        return None\n",
        "\n",
        "\n",
        "    def search_max_length(self, text: str, start: int) -> List[Tuple[str, int, int]]:\n",
        "        \"\"\"Finds all valid words, including overlapping ones, without skipping characters.\"\"\"\n",
        "        result = []\n",
        "\n",
        "        for i in range(start, len(text)):  # Start search at each position\n",
        "            node = self.root\n",
        "            for j in range(i, len(text)):  # Continue searching for words from `i`\n",
        "                char = text[j]\n",
        "                if char not in node.children:\n",
        "                    break  # Stop if the character is not in the Trie\n",
        "\n",
        "                node = node.children[char]\n",
        "\n",
        "                if node.is_end_of_word:\n",
        "                    match = (node.original_string, i, j + 1)  # (word, start index, end index)\n",
        "                    result.append(match)  # Store all matches\n",
        "\n",
        "        return result  # Return all matches\n",
        "\n",
        "\n",
        "\n",
        "    def get_raw_text(self, normalized_text):\n",
        "        return self.original_names.get(normalized_text, normalized_text)\n",
        "\n",
        "def generate_prefixed_variations(location_name: str, category: str) -> Tuple[List[str], str]:\n",
        "    \"\"\"Generate prefixed variations ONLY for wards and districts, and store variations per category.\"\"\"\n",
        "    variations = []\n",
        "\n",
        "    if category not in variation_map:\n",
        "        variation_map[category] = {}\n",
        "\n",
        "    normalized_name = normalize_text_but_keep_accent(location_name)\n",
        "\n",
        "    if normalized_name.isdigit():  # Only generate prefixes for wards and districts\n",
        "        padded_name = normalized_name.zfill(2) if len(normalized_name) == 1 else normalized_name\n",
        "        unpadded_name = str(int(normalized_name))  # \"01\" -> \"1\", keeps \"11\" as \"11\"\n",
        "\n",
        "        all_number_forms = {padded_name, unpadded_name}  # Set ensures unique values\n",
        "\n",
        "        variations = [prefix + num for num in all_number_forms for prefix in DIGIT_CASES[category]]\n",
        "        # print(variations)\n",
        "    elif normalized_name in province_short_form:\n",
        "        variations = [normalized_name, province_short_form[normalized_name]]\n",
        "    else:\n",
        "        variations = [normalized_name]\n",
        "\n",
        "    non_accents_variations = [normalize_text_and_remove_accent(variation) for variation in variations]\n",
        "    variations.extend(non_accents_variations)\n",
        "\n",
        "    # Store variations per category\n",
        "    variation_map[category][normalized_name] = variations\n",
        "    return variations, normalized_name\n",
        "\n",
        "\n",
        "def load_databases(filenames: Dict[str, str], tries) -> Dict[str, Trie]:\n",
        "    \"\"\"Load multiple database files into separate Tries with prefixed variations.\"\"\"\n",
        "    for category, filename in filenames.items():\n",
        "        trie = Trie()\n",
        "        try:\n",
        "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "                for line in file:\n",
        "                    load_line(line, trie, category)\n",
        "            tries[category] = trie\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File {filename} not found!\")\n",
        "            tries[category] = Trie()\n",
        "    return tries\n",
        "\n",
        "def load_line(line, trie, category):\n",
        "    location_name = line.strip()\n",
        "    if location_name == \"\":\n",
        "        return\n",
        "\n",
        "    prefixed_variations, normalized_text = generate_prefixed_variations(location_name, category)\n",
        "\n",
        "    for variant in prefixed_variations:\n",
        "        trie.original_names[variant] = location_name\n",
        "        trie.insert(variant)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q2Y4DLm-eKWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Autocorrect\n",
        "# Both Normalized and Un-normalized words are loaded to Trie\n",
        "# Inputs are misspelled words (misspelled_words)\n",
        "# Outputs are corrected words via Database\n",
        "# If Output is 'null', please check Database if it really exists\n",
        "# Two parameters can be tuned: COSINE_SIMILARITY_THRESHOLD = 0.75\n",
        "#                                     distance = damerau_levenshtein(word_normalized, candidate_normalized, max_distance=3)\n",
        "# Search Priority is set as province > district > ward\n",
        "\n",
        "####################################################################\n",
        "import time\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "import editdistance\n",
        "\n",
        "# Define category ranking\n",
        "CATEGORY_PRIORITY = {\"province\": 1, \"district\": 2, \"ward\": 3}  # Lower number = higher priority\n",
        "COSINE_SIMILARITY_THRESHOLD = 0.75  # If similarity < 0.7, return \"null\", the less value - the less strict\n",
        "MAX_VALID_EDIT_DISTANCE = 3\n",
        "\n",
        "# Damerau-Levenshtein Distance with max_distance\n",
        "# def damerau_levenshtein(s1, s2, max_distance=3):\n",
        "#     len_s1, len_s2 = len(s1), len(s2)\n",
        "#     if abs(len_s1 - len_s2) > max_distance:\n",
        "#         return max_distance + 1\n",
        "#\n",
        "#     prev_row = list(range(len_s2 + 1))\n",
        "#     curr_row = [0] * (len_s2 + 1)\n",
        "#\n",
        "#     for i in range(1, len_s1 + 1):\n",
        "#         curr_row[0] = i\n",
        "#         min_row_value = i\n",
        "#\n",
        "#         for j in range(1, len_s2 + 1):\n",
        "#             cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "#             curr_row[j] = min(curr_row[j - 1] + 1, prev_row[j] + 1, prev_row[j - 1] + cost)\n",
        "#             if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
        "#                 curr_row[j] = min(curr_row[j], prev_row[j - 2] + 1)\n",
        "#\n",
        "#             min_row_value = min(min_row_value, curr_row[j])\n",
        "#\n",
        "#         if min_row_value > max_distance:\n",
        "#             return max_distance + 1\n",
        "#\n",
        "#         prev_row, curr_row = curr_row, prev_row\n",
        "#\n",
        "#     return prev_row[len_s2]\n",
        "\n",
        "# Cosine Similarity Function\n",
        "def cosine_similarity(s1, s2):\n",
        "    vec1, vec2 = Counter(s1), Counter(s2)\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    dot_product = sum(vec1[x] * vec2[x] for x in intersection)\n",
        "    magnitude1 = math.sqrt(sum(vec1[x] ** 2 for x in vec1.keys()))\n",
        "    magnitude2 = math.sqrt(sum(vec2[x] ** 2 for x in vec2.keys()))\n",
        "    return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
        "\n",
        "# Autocorrection with Category Priority, Damerau-Levenshtein Distance & Cosine Similarity Check\n",
        "def autocorrect(word_normalized, trie: Trie, category):\n",
        "    best_distance = float(\"inf\")\n",
        "    matches = []\n",
        "    for candidate_normalized in trie.all_words:\n",
        "        distance = editdistance.distance(word_normalized, candidate_normalized)\n",
        "\n",
        "        # Prioritize lower edit distance & higher-ranked category (Province > District > Ward)\n",
        "        if distance < min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
        "            matches = [candidate_normalized]\n",
        "            best_distance = distance\n",
        "        elif distance == min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
        "            matches.append(candidate_normalized)\n",
        "\n",
        "    # Check Cosine Similarity Threshold\n",
        "    best_match = \"\"\n",
        "    best_similarity = 0\n",
        "    for match in matches:\n",
        "        p = cosine_similarity(word_normalized, match)\n",
        "        if p > COSINE_SIMILARITY_THRESHOLD and  p > best_similarity:\n",
        "            best_similarity = p\n",
        "            best_match = match\n",
        "\n",
        "    return best_match\n"
      ],
      "metadata": {
        "id": "_Dze6_i5eLMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Searcher\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "\n",
        "def search_locations_in_trie(tries: Dict[str, Trie], input_text: str, results) -> Tuple[Dict[str, Optional[str]], str]:\n",
        "    matched_positions = set()\n",
        "\n",
        "    remaining_chars = list(input_text)\n",
        "\n",
        "    for category, reverse in [(\"province\", True), (\"district\", False), (\"ward\", False)]:\n",
        "        if results[category] != \"\":\n",
        "            continue\n",
        "        match = search_part(tries[category], input_text, matched_positions, remaining_chars, reverse)\n",
        "        res = match[0]\n",
        "        if match and res:\n",
        "            input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
        "        results[category] = res\n",
        "\n",
        "    return results, input_text\n",
        "\n",
        "def search_locations_in_segments(tries: Dict[str, Trie], segments: [], results) -> Tuple[Dict[str, Optional[str]], str]:\n",
        "    for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
        "        if results[category] != \"\":\n",
        "            continue\n",
        "\n",
        "        res = search_in_segment(segments, tries[category], category, reverse)\n",
        "        results[category] = res\n",
        "\n",
        "    return results, segments\n",
        "\n",
        "def search_in_trie(trie, input_text, matched_positions, remaining_chars, reverse):\n",
        "    match = search_part(trie, input_text, matched_positions, remaining_chars, reverse)\n",
        "    res = match[0]\n",
        "    if match and res:\n",
        "        input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
        "    return res, input_text\n",
        "\n",
        "def search_in_segment(segments, trie, category, reverse=False):\n",
        "    if reverse:\n",
        "        segments.reverse()\n",
        "\n",
        "    for seg in segments:\n",
        "        word = autocorrect(seg, trie, category)\n",
        "        if word == \"\":\n",
        "            continue\n",
        "        segments.remove(seg)\n",
        "        return word\n",
        "\n",
        "    if reverse:\n",
        "        segments.reverse()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def search_part(trie, input_text, matched_positions, remaining_chars, reversed=False):\n",
        "    it_range = range(len(input_text))\n",
        "    matched_intervals = []\n",
        "    if reversed:\n",
        "        it_range = it_range.__reversed__()\n",
        "    for i in it_range:\n",
        "        if i in matched_positions:\n",
        "            continue\n",
        "        matches = trie.search_max_length(input_text, i)\n",
        "\n",
        "        if matches:\n",
        "            best_match = max(matches, key=lambda x: (x[2], x[2] - x[1]))  # Prioritize the end and length\n",
        "            # Check if the best match is actually the longest match covering the end of the string\n",
        "            if any(match[1] <= len(input_text) - 1 <= match[2] for match in matches):\n",
        "                best_match = max(matches, key=lambda x: (x[2], x[2] - x[1]))\n",
        "            # Ensure no overlap with previously matched regions\n",
        "            start, end = best_match[1], best_match[2]\n",
        "            if any(s <= start < e or s < end <= e for s, e in matched_intervals):\n",
        "                continue  # Skip overlapping matches\n",
        "            return best_match\n",
        "    return \"\", None, None"
      ],
      "metadata": {
        "id": "hPtboSYleP-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you MUST change this cell\n",
        "# New methods / functions must be written under class Solution.\n",
        "\n",
        "import re\n",
        "class Solution:\n",
        "    def __init__(self):\n",
        "        # list provice, district, ward for private test, do not change for any reason (these file will be provided later with this exact name)\n",
        "\n",
        "        self.province_path = 'list_province.txt'\n",
        "        self.district_path = 'list_district.txt'\n",
        "        self.ward_path = 'list_ward.txt'\n",
        "\n",
        "        self.tries = {}\n",
        "        load_databases({\n",
        "            \"province\": self.province_path,\n",
        "            \"district\": self.district_path,\n",
        "            \"ward\": self.ward_path\n",
        "        }, self.tries)\n",
        "\n",
        "        self.variation_map = variation_map\n",
        "\n",
        "\n",
        "    def process(self, s: str):\n",
        "        # Preprocess\n",
        "        s_copy = s[:]\n",
        "\n",
        "        segments = segment_text(s)\n",
        "        input_text = normalize_text_but_keep_accent(\",\".join(segments))\n",
        "\n",
        "        # Start searching\n",
        "        results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
        "\n",
        "        # Search with accents\n",
        "        result, remaining_text = search_locations_in_trie(self.tries, input_text, results)\n",
        "\n",
        "        # If the province/district/ward not found, search without accents\n",
        "        # remaining_text = normalize_text_and_remove_accent(remaining_text)\n",
        "        # result, remaining_text = search_locations_in_trie(self.tries, remaining_text, results)\n",
        "\n",
        "        # If the province/district/ward not found, search by segments\n",
        "        segments = segment_text(remaining_text, False)\n",
        "        result, remaining_text = search_locations_in_segments(self.tries, segments, results)\n",
        "\n",
        "        result =  {\n",
        "            \"province\": self.tries[\"province\"].get_raw_text(result[\"province\"]),\n",
        "            \"district\": self.tries[\"district\"].get_raw_text(result[\"district\"]),\n",
        "            \"ward\": self.tries[\"ward\"].get_raw_text(result[\"ward\"]),\n",
        "        }\n",
        "\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "xtwG3tBDzMLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: DO NOT change this cell\n",
        "# This cell is for downloading private test\n",
        "!rm -rf test.json\n",
        "# this link is public test\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB/view?usp=sharing -O test.json"
      ],
      "metadata": {
        "id": "7Sdb3ddTr1Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRECT TESTS\n",
        "groups_province = {}\n",
        "groups_district = {'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'kbang': ['Kbang', 'KBang'], 'quy nhơn': ['Qui Nhơn', 'Quy Nhơn']}\n",
        "groups_ward = {'ái nghĩa': ['ái Nghĩa', 'Ái Nghĩa'], 'ái quốc': ['ái Quốc', 'Ái Quốc'], 'ái thượng': ['ái Thượng', 'Ái Thượng'], 'ái tử': ['ái Tử', 'Ái Tử'], 'ấm hạ': ['ấm Hạ', 'Ấm Hạ'], 'an ấp': ['An ấp', 'An Ấp'], 'ẳng cang': ['ẳng Cang', 'Ẳng Cang'], 'ẳng nưa': ['ẳng Nưa', 'Ẳng Nưa'], 'ẳng tở': ['ẳng Tở', 'Ẳng Tở'], 'an hòa': ['An Hoà', 'An Hòa'], 'ayun': ['Ayun', 'AYun'], 'bắc ái': ['Bắc ái', 'Bắc Ái'], 'bảo ái': ['Bảo ái', 'Bảo Ái'], 'bình hòa': ['Bình Hoà', 'Bình Hòa'], 'châu ổ': ['Châu ổ', 'Châu Ổ'], 'chư á': ['Chư á', 'Chư Á'], 'chư rcăm': ['Chư Rcăm', 'Chư RCăm'], 'cộng hòa': ['Cộng Hoà', 'Cộng Hòa'], 'cò nòi': ['Cò  Nòi', 'Cò Nòi'], 'đại ân 2': ['Đại Ân  2', 'Đại Ân 2'], 'đak ơ': ['Đak ơ', 'Đak Ơ'], \"đạ m'ri\": [\"Đạ M'ri\", \"Đạ M'Ri\"], 'đông hòa': ['Đông Hoà', 'Đông Hòa'], 'đồng ích': ['Đồng ích', 'Đồng Ích'], 'hải châu i': ['Hải Châu  I', 'Hải Châu I'], 'hải hòa': ['Hải Hoà', 'Hải Hòa'], 'hành tín đông': ['Hành Tín  Đông', 'Hành Tín Đông'], 'hiệp hòa': ['Hiệp Hoà', 'Hiệp Hòa'], 'hòa bắc': ['Hoà Bắc', 'Hòa Bắc'], 'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'hòa châu': ['Hoà Châu', 'Hòa Châu'], 'hòa hải': ['Hoà Hải', 'Hòa Hải'], 'hòa hiệp trung': ['Hoà Hiệp Trung', 'Hòa Hiệp Trung'], 'hòa liên': ['Hoà Liên', 'Hòa Liên'], 'hòa lộc': ['Hoà Lộc', 'Hòa Lộc'], 'hòa lợi': ['Hoà Lợi', 'Hòa Lợi'], 'hòa long': ['Hoà Long', 'Hòa Long'], 'hòa mạc': ['Hoà Mạc', 'Hòa Mạc'], 'hòa minh': ['Hoà Minh', 'Hòa Minh'], 'hòa mỹ': ['Hoà Mỹ', 'Hòa Mỹ'], 'hòa phát': ['Hoà Phát', 'Hòa Phát'], 'hòa phong': ['Hoà Phong', 'Hòa Phong'], 'hòa phú': ['Hoà Phú', 'Hòa Phú'], 'hòa phước': ['Hoà Phước', 'Hòa Phước'], 'hòa sơn': ['Hoà Sơn', 'Hòa Sơn'], 'hòa tân': ['Hoà Tân', 'Hòa Tân'], 'hòa thuận': ['Hoà Thuận', 'Hòa Thuận'], 'hòa tiến': ['Hoà Tiến', 'Hòa Tiến'], 'hòa trạch': ['Hoà Trạch', 'Hòa Trạch'], 'hòa vinh': ['Hoà Vinh', 'Hòa Vinh'], 'hương hòa': ['Hương Hoà', 'Hương Hòa'], 'ích hậu': ['ích Hậu', 'Ích Hậu'], 'ít ong': ['ít Ong', 'Ít Ong'], 'khánh hòa': ['Khánh Hoà', 'Khánh Hòa'], 'krông á': ['Krông Á', 'KRông á'], 'lộc hòa': ['Lộc Hoà', 'Lộc Hòa'], 'minh hòa': ['Minh Hoà', 'Minh Hòa'], 'mường ải': ['Mường ải', 'Mường Ải'], 'mường ẳng': ['Mường ẳng', 'Mường Ẳng'], 'nậm ét': ['Nậm ét', 'Nậm Ét'], 'nam hòa': ['Nam Hoà', 'Nam Hòa'], 'na ư': ['Na ư', 'Na Ư'], 'ngã sáu': ['Ngã sáu', 'Ngã Sáu'], 'nghi hòa': ['Nghi Hoà', 'Nghi Hòa'], 'nguyễn úy': ['Nguyễn Uý', 'Nguyễn úy', 'Nguyễn Úy'], 'nhân hòa': ['Nhân Hoà', 'Nhân Hòa'], 'nhơn hòa': ['Nhơn Hoà', 'Nhơn Hòa'], 'nhơn nghĩa a': ['Nhơn nghĩa A', 'Nhơn Nghĩa A'], 'phúc ứng': ['Phúc ứng', 'Phúc Ứng'], 'phước hòa': ['Phước Hoà', 'Phước Hòa'], 'sơn hóa': ['Sơn Hoá', 'Sơn Hóa'], 'tạ an khương đông': ['Tạ An Khương  Đông', 'Tạ An Khương Đông'], 'tạ an khương nam': ['Tạ An Khương  Nam', 'Tạ An Khương Nam'], 'tăng hòa': ['Tăng Hoà', 'Tăng Hòa'], 'tân hòa': ['Tân Hoà', 'Tân Hòa'], 'tân hòa thành': ['Tân Hòa  Thành', 'Tân Hòa Thành'], 'tân khánh trung': ['Tân  Khánh Trung', 'Tân Khánh Trung'], 'tân lợi': ['Tân lợi', 'Tân Lợi'], 'thái hòa': ['Thái Hoà', 'Thái Hòa'], 'thiết ống': ['Thiết ống', 'Thiết Ống'], 'thuận hòa': ['Thuận Hoà', 'Thuận Hòa'], 'thượng ấm': ['Thượng ấm', 'Thượng Ấm'], 'thụy hương': ['Thuỵ Hương', 'Thụy Hương'], 'thủy xuân': ['Thuỷ Xuân', 'Thủy Xuân'], 'tịnh ấn đông': ['Tịnh ấn Đông', 'Tịnh Ấn Đông'], 'tịnh ấn tây': ['Tịnh ấn Tây', 'Tịnh Ấn Tây'], 'triệu ái': ['Triệu ái', 'Triệu Ái'], 'triệu ẩu': ['Triệu ẩu', 'Triệu Ẩu'], 'trung hòa': ['Trung Hoà', 'Trung Hòa'], 'trung ý': ['Trung ý', 'Trung Ý'], 'tùng ảnh': ['Tùng ảnh', 'Tùng Ảnh'], 'úc kỳ': ['úc Kỳ', 'Úc Kỳ'], 'ứng hòe': ['ứng Hoè', 'Ứng Hoè'], 'vĩnh hòa': ['Vĩnh Hoà', 'Vĩnh Hòa'], 'vũ hòa': ['Vũ Hoà', 'Vũ Hòa'], 'xuân ái': ['Xuân ái', 'Xuân Ái'], 'xuân áng': ['Xuân áng', 'Xuân Áng'], 'xuân hòa': ['Xuân Hoà', 'Xuân Hòa'], 'xuất hóa': ['Xuất Hoá', 'Xuất Hóa'], 'ỷ la': ['ỷ La', 'Ỷ La']}\n",
        "groups_ward.update({1: ['1', '01'], 2: ['2', '02'], 3: ['3', '03'], 4: ['4', '04'], 5: ['5', '05'], 6: ['6', '06'], 7: ['7', '07'], 8: ['8', '08'], 9: ['9', '09']})\n",
        "def to_same(groups):\n",
        "    same = {ele: k for k, v in groups.items() for ele in v}\n",
        "    return same\n",
        "same_province = to_same(groups_province)\n",
        "same_district = to_same(groups_district)\n",
        "same_ward = to_same(groups_ward)\n",
        "def normalize(text, same_dict):\n",
        "    return same_dict.get(text, text)"
      ],
      "metadata": {
        "id": "3KN8RZL6tFzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEAM_NAME = 'Team04'  # This should be your team name\n",
        "EXCEL_FILE = f'{TEAM_NAME}.xlsx'\n",
        "\n",
        "import json\n",
        "import time\n",
        "with open('test.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "summary_only = True\n",
        "df = []\n",
        "solution = Solution()\n",
        "timer = []\n",
        "correct = 0\n",
        "for test_idx, data_point in enumerate(data):\n",
        "    address = data_point[\"text\"]\n",
        "\n",
        "    ok = 0\n",
        "    try:\n",
        "        answer = data_point[\"result\"]\n",
        "        answer[\"province_normalized\"] = normalize(answer[\"province\"], same_province)\n",
        "        answer[\"district_normalized\"] = normalize(answer[\"district\"], same_district)\n",
        "        answer[\"ward_normalized\"] = normalize(answer[\"ward\"], same_ward)\n",
        "\n",
        "        start = time.perf_counter_ns()\n",
        "        result = solution.process(address)\n",
        "        finish = time.perf_counter_ns()\n",
        "        timer.append(finish - start)\n",
        "        result[\"province_normalized\"] = normalize(result[\"province\"], same_province)\n",
        "        result[\"district_normalized\"] = normalize(result[\"district\"], same_district)\n",
        "        result[\"ward_normalized\"] = normalize(result[\"ward\"], same_ward)\n",
        "\n",
        "        province_correct = int(answer[\"province_normalized\"] == result[\"province_normalized\"])\n",
        "        district_correct = int(answer[\"district_normalized\"] == result[\"district_normalized\"])\n",
        "        ward_correct = int(answer[\"ward_normalized\"] == result[\"ward_normalized\"])\n",
        "        ok = province_correct + district_correct + ward_correct\n",
        "\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            result[\"province\"],\n",
        "            answer[\"province_normalized\"],\n",
        "            result[\"province_normalized\"],\n",
        "            province_correct,\n",
        "            answer[\"district\"],\n",
        "            result[\"district\"],\n",
        "            answer[\"district_normalized\"],\n",
        "            result[\"district_normalized\"],\n",
        "            district_correct,\n",
        "            answer[\"ward\"],\n",
        "            result[\"ward\"],\n",
        "            answer[\"ward_normalized\"],\n",
        "            result[\"ward_normalized\"],\n",
        "            ward_correct,\n",
        "            ok,\n",
        "            timer[-1] / 1_000_000_000,\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        print(f\"{answer = }\")\n",
        "        print(f\"{result = }\")\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"province_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"district\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"district_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"ward\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"ward_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            0,\n",
        "            0,\n",
        "        ])\n",
        "        # any failure count as a zero correct\n",
        "        pass\n",
        "    correct += ok\n",
        "\n",
        "\n",
        "    if not summary_only:\n",
        "        # responsive stuff\n",
        "        print(f\"Test {test_idx:5d}/{len(data):5d}\")\n",
        "        print(f\"Correct: {ok}/3\")\n",
        "        print(f\"Time Executed: {timer[-1] / 1_000_000_000:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"-\"*30)\n",
        "total = len(data) * 3\n",
        "score_scale_10 = round(correct / total * 10, 2)\n",
        "if len(timer) == 0:\n",
        "    timer = [0]\n",
        "max_time_sec = round(max(timer) / 1_000_000_000, 4)\n",
        "avg_time_sec = round((sum(timer) / len(timer)) / 1_000_000_000, 4)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df2 = pd.DataFrame(\n",
        "    [[correct, total, score_scale_10, max_time_sec, avg_time_sec]],\n",
        "    columns=['correct', 'total', 'score / 10', 'max_time_sec', 'avg_time_sec',],\n",
        ")\n",
        "\n",
        "columns = [\n",
        "    'ID',\n",
        "    'text',\n",
        "    'province',\n",
        "    'province_student',\n",
        "    'province_normalized',\n",
        "    'province_student_normalized',\n",
        "    'province_correct',\n",
        "    'district',\n",
        "    'district_student',\n",
        "    'district_normalized',\n",
        "    'district_student_normalized',\n",
        "    'district_correct',\n",
        "    'ward',\n",
        "    'ward_student',\n",
        "    'ward_normalized',\n",
        "    'ward_student_normalized',\n",
        "    'ward_correct',\n",
        "    'total_correct',\n",
        "    'time_sec',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "df.columns = columns\n",
        "\n",
        "print(f'{TEAM_NAME = }')\n",
        "print(f'{EXCEL_FILE = }')\n",
        "print(df2)\n",
        "\n",
        "!pip install xlsxwriter\n",
        "writer = pd.ExcelWriter(EXCEL_FILE, engine='xlsxwriter')\n",
        "df2.to_excel(writer, index=False, sheet_name='summary')\n",
        "df.to_excel(writer, index=False, sheet_name='details')\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "hjO6FFcA0DYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}