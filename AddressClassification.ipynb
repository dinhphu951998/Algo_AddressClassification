{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# NOTE: you CAN change this cell\n",
    "# If you want to use your own database, download it here\n",
    "# !gdown ...\n",
    "!rm -rf list_province.txt\n",
    "!rm -rf list_district.txt\n",
    "!rm -rf list_ward.txt\n",
    "\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1FC5Kb1iL_ElLboQ_yuJt-RyujZWVBe5N/view?usp=sharing -O list_province.txt\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1mLEuPqn_01ffMGnDy2fntHhpWSNyYHSW/view?usp=sharing -O list_district.txt\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1LmCBNSqf2fY4lcphAFgQlMR_GkCwSLkk/view?usp=sharing -O list_ward.txt"
   ],
   "metadata": {
    "id": "i20WfB6lqiUy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "209c7e71-8c91-4802-f702-11b8f7b2ab57"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FC5Kb1iL_ElLboQ_yuJt-RyujZWVBe5N\n",
      "To: /content/list_province.txt\n",
      "100% 5.07k/5.07k [00:00<00:00, 18.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mLEuPqn_01ffMGnDy2fntHhpWSNyYHSW\n",
      "To: /content/list_district.txt\n",
      "100% 4.72k/4.72k [00:00<00:00, 16.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LmCBNSqf2fY4lcphAFgQlMR_GkCwSLkk\n",
      "To: /content/list_ward.txt\n",
      "100% 4.79k/4.79k [00:00<00:00, 16.5MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "  # NOTE: you CAN change this cell\n",
    "# Add more to your needs\n",
    "# you must place ALL pip install here\n",
    "!pip install editdistance"
   ],
   "metadata": {
    "id": "J8znFuZTzwoS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55949e7e-a0ab-463b-b08b-3d8cebaf3947"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# NOTE: you CAN change this cell\n",
    "# import your library here\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from queue import Queue\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import editdistance\n",
    "from gettext import textdomain\n",
    "from numpy import character\n"
   ],
   "metadata": {
    "id": "AodaIxYa32hT"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "id": "QAsaktzUnFy5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from gettext import textdomain\n",
    "\n",
    "import unicodedata\n",
    "from numpy import character\n",
    "\n",
    "SAFE_CASES = [\n",
    "                #\"TP.\", \"T.P\", \"F.\", \"Th√†nh ph·ªë\", \"Th√†nhPh·ªë\", \"TP \", \" TP\",\n",
    "               \"T·ªânh\", \" T \", #\"T·ªân\",  \",T \", \"T.\",\n",
    "               #\"Qu·∫≠n\", \"Q.\", \" Q \", \",Q \", -> qu·∫≠n 5 sau khi remove s·∫Ω th√†nh 5 v√† kh√¥ng t√¨m ra\n",
    "               \"Huy·ªán\", \"hy·ªán\", #\"H.\", \" H \", \",H \",\n",
    "               # \"Ph∆∞·ªùng\", \"F\",  \"P.\", \" P \", \",P \",\n",
    "               \"Th·ªã x√£\", \"Th·ªãX√£\", \"X√£\", # \"X.\", \" X \", \"X \", \",X \",\n",
    "                \"Th·ªã tr·∫•n\", \"Th·ªãTr·∫•n\", #\"T.T\",\n",
    "                \"khu ph·ªë\", \"KP\", \"KhuPh·ªë\", \"Khu pho\", \"KhuPho\", # -> KP5 b·ªã nh·∫ßm th√†nh P5\n",
    "                \"·∫§p\"\n",
    "               ]\n",
    "\n",
    "\n",
    "ALL_PREFIXES = [\"TP.\", \"T.P\", \"F.\", \"Th√†nh ph·ªë\", \"Th√†nhPh·ªë\", \"TP \", \" TP\",\n",
    "               \"T·ªânh\", \"T·ªân\",\"T.\", \" T \",  #\",T \",\n",
    "               \"Qu·∫≠n\", \"Q.\", \" Q \", \",Q \",\n",
    "               \"Huy·ªán\", \"hy·ªán\", \"H.\", \" H \",  #\",H \",\n",
    "                #\"KP\", # KP. contains P.\n",
    "               \"khu ph·ªë\",  \"KhuPh·ªë\", \"Khu pho\", \"KhuPho\",\n",
    "               \"Ph∆∞·ªùng\", \"P.\", \"F\", \" P \",  #\",P \",\n",
    "               \"X.\", \"Th·ªã x√£\", \"Th·ªãX√£\", \"X√£\",  #\" X \", \"X \", \",X \",\n",
    "                \"Th·ªã tr·∫•n\", \"Th·ªãTr·∫•n\", \"T.T\",\n",
    "               \"-\"\n",
    "                ]\n",
    "\n",
    "vietnamese_dict = {\n",
    "    \"a\": [\"√†\", \"√°\", \"·∫°\", \"·∫£\", \"√£\", \"√¢\", \"·∫ß\", \"·∫•\", \"·∫≠\", \"·∫©\", \"·∫´\", \"ƒÉ\", \"·∫±\", \"·∫Ø\", \"·∫∑\", \"·∫≥\", \"·∫µ\"],\n",
    "    \"d\": [\"ƒë\"],\n",
    "    \"e\": [\"√®\", \"√©\", \"·∫π\", \"·∫ª\", \"·∫Ω\", \"√™\", \"·ªÅ\", \"·∫ø\", \"·ªá\", \"·ªÉ\", \"·ªÖ\"],\n",
    "    \"i\": [\"√¨\", \"√≠\", \"·ªã\", \"·ªâ\", \"ƒ©\"],\n",
    "    \"o\": [\"√≤\", \"√≥\", \"·ªç\", \"·ªè\", \"√µ\", \"√¥\", \"·ªì\", \"·ªë\", \"·ªô\", \"·ªï\", \"·ªó\", \"∆°\", \"·ªù\", \"·ªõ\", \"·ª£\", \"·ªü\", \"·ª°\"],\n",
    "    \"u\": [\"√π\", \"√∫\", \"·ª•\", \"·ªß\", \"≈©\", \"∆∞\", \"·ª´\", \"·ª©\", \"·ª±\", \"·ª≠\", \"·ªØ\"],\n",
    "    \"y\": [\"·ª≥\", \"√Ω\", \"·ªµ\", \"·ª∑\", \"·ªπ\"],\n",
    "\n",
    "}\n",
    "\n",
    "reversed_vietnamese_dict = {}\n",
    "\n",
    "wrong_accents = {\n",
    "    \"o√†\": \"√≤a\", \"o√°\": \"√≥a\", \"o·∫°\": \"·ªça\", \"o√£\": \"√µa\", \"o·∫£\": \"·ªèa\",\n",
    "    \"qui\": \"quy\",\n",
    "}\n",
    "\n",
    "def common_normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # text = text.replace(\",\", \"\")  # replace for T,√¢,n,B,√¨,n,h Dƒ© An B√¨nh D∆∞∆°ng\n",
    "    # text = text.replace(\".\", \"\")\n",
    "    for case in SAFE_CASES:\n",
    "        text = re.sub(re.escape(case), ',', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \",\", text)  # Remove spaces\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text\n",
    "\n",
    "def normalize_text_and_remove_accent(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "    text = common_normalize(text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.replace(\"ƒë\", \"d\")\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "    # text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "    return text\n",
    "\n",
    "def normalize_text_but_keep_vietnamese_alphabet(text: str) -> str:\n",
    "    text = common_normalize(text)\n",
    "    for base_char, variations in vietnamese_dict.items():\n",
    "        for char in variations:\n",
    "            reversed_vietnamese_dict[char] = base_char\n",
    "\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in reversed_vietnamese_dict:\n",
    "            result.append(reversed_vietnamese_dict[char])\n",
    "        else:\n",
    "            result.append(char)\n",
    "\n",
    "    text = \"\".join(result)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "    text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "    return text\n",
    "\n",
    "def normalize_text_but_keep_accent(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "    text = common_normalize(text)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    for wrong, correct in wrong_accents.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "\n",
    "    return text\n",
    "\n",
    "def segment_text(s, safe=True):\n",
    "    text = s[:]\n",
    "\n",
    "    prefixes = SAFE_CASES if safe else ALL_PREFIXES\n",
    "\n",
    "    for p in prefixes:\n",
    "        text = re.sub(re.escape(p), ',', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # X·ª≠ l√Ω d·∫•u \"-\" trong t√™n (v√≠ d·ª•: \"Ng-T-\" -> \"Ng T \")\n",
    "    text = re.sub(r'[.\\-]', ' ', text)\n",
    "\n",
    "    # T√°ch c·ª•m ƒë·ªãa ch·ªâ\n",
    "    segments = [seg.strip() for seg in text.split(',') if seg.strip()]\n",
    "    #\n",
    "    # print()\n",
    "    # print(f\"'{s}'  -->  {segments}\")\n",
    "    return segments\n",
    "\n",
    "\n",
    "# print(normalize_text_but_keep_accent(\"T18,C·∫©m B√¨nh, C·∫©m Ph·∫£, Qu·∫£ng Ninh\"))\n",
    "# print(normalize_text_but_keep_vietnamese(\"Th√¥n ƒê·ªìng L·ª±c Ho√†ng L√¢u, Tam D∆∞∆°ng, Vƒ©nh Ph√∫c\"))\n",
    "# print(normalize_text_but_keep_vietnamese(\"Tam ƒê∆∞·ªùng, Tam ƒê∆∞·ªùng, Lai Ch√¢u\"))\n",
    "\n"
   ],
   "metadata": {
    "id": "qTASEb_BnHs4"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Index Analyzer"
   ],
   "metadata": {
    "id": "mBBRJNQLmwNc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from queue import Queue\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# from Utils import *\n",
    "locality_map = {}\n",
    "current_id = 0\n",
    "\n",
    "# SPECIAL_CASES = [\"x√£\", \"x.\", \"huy·ªán\", \"t·ªânh\", \"t.\",\n",
    "#                  \"tp\", \"th√†nh ph·ªë\", \"th√†nhph·ªë\"]\n",
    "\n",
    "# Prefixes for wards and districts to expand possible matches\n",
    "# DIGIT_CASES = {\n",
    "#     \"ward\": [\"p\", \"ph∆∞·ªùng\"],\n",
    "#     \"district\": [\"q\", \"qu·∫≠n\"],\n",
    "# }\n",
    "SPECIAL_CASES = [\"x√£\", \"x.\", \"huy·ªán\", \"t·ªânh\", \"t.\", \"s·ªë nh√†\", \"th·ªã tr·∫•n\", \"·∫•p\",\n",
    "                 \"h.\", \"khu ph·ªë\",\"tp\", \"th√†nh ph·ªë\", \"th√†nhph·ªë\"]\n",
    "WARD_CASES = [\"p\", \"p.\", \"ph∆∞·ªùng\"]\n",
    "DISTRICT_CASES = [\"q\", \"q.\", \"qu·∫≠n\"]\n",
    "\n",
    "# Dictionary to store generated variations for tracing back\n",
    "variation_map: Dict[str, dict] = {}\n",
    "\n",
    "province_short_form = {\n",
    "    \"h·ªìch√≠minh\":\"hcm\",\n",
    "    \"b√†r·ªãav≈©ngt√†u\":\"brvt\",\n",
    "}\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.original_string: Optional[str] = None\n",
    "\n",
    "\n",
    "class Trie:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        self.all_words = set()\n",
    "        self.original_names: Dict[str, str] = {} #normalize -> raw\n",
    "\n",
    "    def insert(self, normalized_word: str):\n",
    "        \"\"\"Insert a normalized word into the trie with a reference to the original.\"\"\"\n",
    "        node = self.root\n",
    "        for i, char in enumerate(normalized_word):\n",
    "\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "\n",
    "            node = node.children[char]\n",
    "\n",
    "        node.is_end_of_word = True\n",
    "        node.original_string = normalized_word\n",
    "\n",
    "        self.all_words.add(normalized_word)\n",
    "\n",
    "    def search(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "        \"\"\"Finds the first valid word from the given start index.\"\"\"\n",
    "        node = self.root\n",
    "        for i in range(start, len(text)):\n",
    "            char = text[i]\n",
    "            if char not in node.children:\n",
    "                break\n",
    "            node = node.children[char]\n",
    "            if node.is_end_of_word:\n",
    "                return (node.original_string, start, i + 1)\n",
    "        return None\n",
    "\n",
    "    def search_max_length(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "        \"\"\"Finds the longest valid word from the given start index.\"\"\"\n",
    "        node = self.root\n",
    "        longest_match = \"\"\n",
    "        longest_length = 0\n",
    "        for i in range(start, len(text)):\n",
    "            char = text[i]\n",
    "            if char not in node.children:\n",
    "                break\n",
    "            node = node.children[char]\n",
    "            if node.is_end_of_word:\n",
    "                current_length = i - start + 1\n",
    "                if current_length > longest_length:\n",
    "                    longest_length = current_length\n",
    "                    longest_match = (node.original_string, start, i + 1)\n",
    "        return longest_match\n",
    "\n",
    "    def get_raw_text(self, normalized_text):\n",
    "        return self.original_names.get(normalized_text, normalized_text)\n",
    "\n",
    "def generate_prefixed_variations(location_name: str, category: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Generate prefixed variations ONLY for wards and districts, and store variations per category.\"\"\"\n",
    "    variations = []\n",
    "\n",
    "    if category not in variation_map:\n",
    "        variation_map[category] = {}\n",
    "\n",
    "    normalized_name = normalize_text_but_keep_accent(location_name)\n",
    "\n",
    "    if normalized_name.isdigit():  # Only generate prefixes for wards and districts\n",
    "        variations = [prefix + normalized_name for prefix in DIGIT_CASES[category]]\n",
    "    elif normalized_name in province_short_form:\n",
    "        variations = [normalized_name, province_short_form[normalized_name]]\n",
    "    else:\n",
    "        variations = [normalized_name]\n",
    "\n",
    "    non_accents_variations = [normalize_text_and_remove_accent(variation) for variation in variations]\n",
    "    variations.extend(non_accents_variations)\n",
    "\n",
    "    # Store variations per category\n",
    "    variation_map[category][normalized_name] = variations\n",
    "    return variations, normalized_name\n",
    "\n",
    "\n",
    "def load_databases(filenames: Dict[str, str], tries) -> Dict[str, Trie]:\n",
    "    \"\"\"Load multiple database files into separate Tries with prefixed variations.\"\"\"\n",
    "    for category, filename in filenames.items():\n",
    "        trie = Trie()\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "                for line in file:\n",
    "                    load_line(line, trie, category)\n",
    "            tries[category] = trie\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {filename} not found!\")\n",
    "            tries[category] = Trie()\n",
    "    return tries\n",
    "\n",
    "def load_line(line, trie, category):\n",
    "    location_name = line.strip()\n",
    "    if location_name == \"\":\n",
    "        return\n",
    "\n",
    "    prefixed_variations, normalized_text = generate_prefixed_variations(location_name, category)\n",
    "\n",
    "    for variant in prefixed_variations:\n",
    "        trie.original_names[variant] = location_name\n",
    "        trie.insert(variant)\n"
   ],
   "metadata": {
    "id": "ypVaUDchmhvc"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrect"
   ],
   "metadata": {
    "id": "TMAkHGhQnXkr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Both Normalized and Un-normalized words are loaded to Trie\n",
    "# Inputs are misspelled words (misspelled_words)\n",
    "# Outputs are corrected words via Database\n",
    "# If Output is 'null', please check Database if it really exists\n",
    "# Two parameters can be tuned: COSINE_SIMILARITY_THRESHOLD = 0.75\n",
    "#                                     distance = damerau_levenshtein(word_normalized, candidate_normalized, max_distance=3)\n",
    "# Search Priority is set as province > district > ward\n",
    "\n",
    "####################################################################\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# from IndexAnalyzer import Trie\n",
    "# from Utils import normalize_text_but_keep_vietnamese_alphabet, normalize_text_but_keep_accent\n",
    "\n",
    "# Define category ranking\n",
    "CATEGORY_PRIORITY = {\"province\": 1, \"district\": 2, \"ward\": 3}  # Lower number = higher priority\n",
    "COSINE_SIMILARITY_THRESHOLD = 0.75  # If similarity < 0.7, return \"null\", the less value - the less strict\n",
    "MAX_VALID_EDIT_DISTANCE = 3\n",
    "\n",
    "# Damerau-Levenshtein Distance with max_distance\n",
    "# def damerau_levenshtein(s1, s2, max_distance=3):\n",
    "#     len_s1, len_s2 = len(s1), len(s2)\n",
    "#     if abs(len_s1 - len_s2) > max_distance:\n",
    "#         return max_distance + 1\n",
    "#\n",
    "#     prev_row = list(range(len_s2 + 1))\n",
    "#     curr_row = [0] * (len_s2 + 1)\n",
    "#\n",
    "#     for i in range(1, len_s1 + 1):\n",
    "#         curr_row[0] = i\n",
    "#         min_row_value = i\n",
    "#\n",
    "#         for j in range(1, len_s2 + 1):\n",
    "#             cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "#             curr_row[j] = min(curr_row[j - 1] + 1, prev_row[j] + 1, prev_row[j - 1] + cost)\n",
    "#             if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "#                 curr_row[j] = min(curr_row[j], prev_row[j - 2] + 1)\n",
    "#\n",
    "#             min_row_value = min(min_row_value, curr_row[j])\n",
    "#\n",
    "#         if min_row_value > max_distance:\n",
    "#             return max_distance + 1\n",
    "#\n",
    "#         prev_row, curr_row = curr_row, prev_row\n",
    "#\n",
    "#     return prev_row[len_s2]\n",
    "\n",
    "# Cosine Similarity Function\n",
    "def cosine_similarity(s1, s2):\n",
    "    vec1, vec2 = Counter(s1), Counter(s2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    dot_product = sum(vec1[x] * vec2[x] for x in intersection)\n",
    "    magnitude1 = math.sqrt(sum(vec1[x] ** 2 for x in vec1.keys()))\n",
    "    magnitude2 = math.sqrt(sum(vec2[x] ** 2 for x in vec2.keys()))\n",
    "    return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
    "\n",
    "# Autocorrection with Category Priority, Damerau-Levenshtein Distance & Cosine Similarity Check\n",
    "def autocorrect(word_normalized, trie: Trie, category):\n",
    "    best_distance = float(\"inf\")\n",
    "    matches = []\n",
    "    for candidate_normalized in trie.all_words:\n",
    "        distance = editdistance.distance(word_normalized, candidate_normalized)\n",
    "\n",
    "        # Prioritize lower edit distance & higher-ranked category (Province > District > Ward)\n",
    "        if distance < min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "            matches = [candidate_normalized]\n",
    "            best_distance = distance\n",
    "        elif distance == min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "            matches.append(candidate_normalized)\n",
    "\n",
    "    # Check Cosine Similarity Threshold\n",
    "    best_match = \"\"\n",
    "    best_similarity = 0\n",
    "    for match in matches:\n",
    "        p = cosine_similarity(word_normalized, match)\n",
    "        if p > COSINE_SIMILARITY_THRESHOLD and  p > best_similarity:\n",
    "            best_similarity = p\n",
    "            best_match = match\n",
    "\n",
    "    return best_match\n"
   ],
   "metadata": {
    "id": "rI1EQ9YAnaFC"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Searcher"
   ],
   "metadata": {
    "id": "XBpTj3Apm3EV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# from IndexAnalyzer import Trie\n",
    "# from Autocorrect import autocorrect\n",
    "# from Utils import normalize_text_and_remove_accent\n",
    "\n",
    "\n",
    "def search_locations_in_trie(tries: Dict[str, Trie], input_text: str, results) -> Tuple[Dict[str, Optional[str]], str]:\n",
    "    matched_positions = set()\n",
    "\n",
    "    remaining_chars = list(input_text)\n",
    "\n",
    "    for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "        if results[category] != \"\":\n",
    "            continue\n",
    "        match = search_part(tries[category], input_text, matched_positions, remaining_chars, reverse)\n",
    "        res = match[0]\n",
    "        if match and res:\n",
    "            input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "        results[category] = res\n",
    "\n",
    "    # remaining_text = normalize_text_and_remove_accent(input_text)\n",
    "    # for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "    #     if results[category] != \"\":\n",
    "    #         continue\n",
    "    #     match = search_part(tries[category], remaining_text, matched_positions, remaining_chars, reverse)\n",
    "    #     res = match[0]\n",
    "    #     if match and res:\n",
    "    #         remaining_text = remaining_text[:match[1]] + \",\" + remaining_text[match[2]:]\n",
    "    #         input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "    #     results[category] = res\n",
    "\n",
    "    return results, input_text\n",
    "\n",
    "def search_locations_in_segments(tries: Dict[str, Trie], segments: [], results) -> Tuple[Dict[str, Optional[str]], str]:\n",
    "    for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "        if results[category] != \"\":\n",
    "            continue\n",
    "\n",
    "        res = search_in_segment(segments, tries[category], category, reverse)\n",
    "        results[category] = res\n",
    "\n",
    "    return results, segments\n",
    "\n",
    "def search_in_trie(trie, input_text, matched_positions, remaining_chars, reverse):\n",
    "    match = search_part(trie, input_text, matched_positions, remaining_chars, reverse)\n",
    "    res = match[0]\n",
    "    if match and res:\n",
    "        input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "    return res, input_text\n",
    "\n",
    "def search_in_segment(segments, trie, category, reverse=False):\n",
    "    if reverse:\n",
    "        segments.reverse()\n",
    "\n",
    "    for seg in segments:\n",
    "        word = autocorrect(seg, trie, category)\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        segments.remove(seg)\n",
    "        return word\n",
    "\n",
    "    if reverse:\n",
    "        segments.reverse()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def search_part(trie, input_text, matched_positions, remaining_chars, reversed=False):\n",
    "    it_range = range(len(input_text))\n",
    "    if reversed:\n",
    "        it_range = it_range.__reversed__()\n",
    "    for i in it_range:\n",
    "        if i in matched_positions:\n",
    "            continue\n",
    "        match = trie.search_max_length(input_text, i)\n",
    "        if match:\n",
    "            return match\n",
    "    return \"\", None, None"
   ],
   "metadata": {
    "id": "ujQ0od5Mm9AC"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution"
   ],
   "metadata": {
    "id": "ImCuX98Mm-JQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # NOTE: you MUST change this cell\n",
    "# # New methods / functions must be written under class Solution.\n",
    "# class TrieNode:\n",
    "#     def __init__(self):\n",
    "#         self.children = {}\n",
    "#         self.is_end_of_word = False\n",
    "#         self.original_string: Optional[str] = None\n",
    "\n",
    "# class Trie:\n",
    "#     def __init__(self):\n",
    "#         self.root = TrieNode()\n",
    "#         self.all_words = set()\n",
    "#         self.original_names: Dict[str, str] = {}\n",
    "\n",
    "#     def insert(self, normalized_word: str):\n",
    "#         \"\"\"Insert a normalized word into the trie with a reference to the original.\"\"\"\n",
    "#         node = self.root\n",
    "#         for i, char in enumerate(normalized_word):\n",
    "#             if char not in node.children:\n",
    "#                 node.children[char] = TrieNode()\n",
    "#             node = node.children[char]\n",
    "#         node.is_end_of_word = True\n",
    "#         node.original_string = normalized_word\n",
    "#         self.all_words.add(normalized_word)\n",
    "\n",
    "#     def search(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "#         \"\"\"Finds the first valid word from the given start index.\"\"\"\n",
    "#         node = self.root\n",
    "#         for i in range(start, len(text)):\n",
    "#             char = text[i]\n",
    "#             if char not in node.children:\n",
    "#                 break\n",
    "#             node = node.children[char]\n",
    "#             if node.is_end_of_word:\n",
    "#                 return (node.original_string, start, i + 1)\n",
    "#         return None\n",
    "\n",
    "#     def search_max_length(self, text: str, start: int) -> Optional[Tuple[str, int, int]]:\n",
    "#         \"\"\"Finds the longest valid word from the given start index.\"\"\"\n",
    "#         node = self.root\n",
    "#         longest_match = \"\"\n",
    "#         longest_length = 0\n",
    "#         for i in range(start, len(text)):\n",
    "#             char = text[i]\n",
    "#             if char not in node.children:\n",
    "#                 break\n",
    "#             node = node.children[char]\n",
    "#             if node.is_end_of_word:\n",
    "#                 current_length = i - start + 1\n",
    "#                 if current_length > longest_length:\n",
    "#                     longest_length = current_length\n",
    "#                     longest_match = (node.original_string, start, i + 1)\n",
    "#         return longest_match\n",
    "\n",
    "#     def get_raw_text(self, normalized_text):\n",
    "#         return self.original_names.get(normalized_text, normalized_text)\n",
    "\n",
    "\n",
    "# # ========================= Solution.py =========================\n",
    "# class Solution:\n",
    "#     SAFE_CASES = [\n",
    "#             \"T·ªânh\", \" T \",\n",
    "#             \"Huy·ªán\", \"hy·ªán\",\n",
    "#             \"Th·ªã x√£\", \"Th·ªãX√£\", \"X√£\",\n",
    "#             \"Th·ªã tr·∫•n\", \"Th·ªãTr·∫•n\",\n",
    "#             \"khu ph·ªë\", \"KP\", \"KhuPh·ªë\", \"Khu pho\", \"KhuPho\",\n",
    "#         ]\n",
    "\n",
    "#     ALL_PREFIXES = [\"TP.\", \"T.P\", \"F.\", \"Th√†nh ph·ªë\", \"Th√†nhPh·ªë\", \"TP \", \" TP\",\n",
    "#                                 \"T·ªânh\", \"T·ªân\", \"T.\", \" T \",\n",
    "#                                 \"Qu·∫≠n\", \"Q.\", \" Q \", \",Q \",\n",
    "#                                 \"Huy·ªán\", \"hy·ªán\", \"H.\", \" H \",\n",
    "#                                 \"khu ph·ªë\", \"KhuPh·ªë\", \"Khu pho\", \"KhuPho\",\n",
    "#                                 \"Ph∆∞·ªùng\", \"P.\", \"F\", \" P \",\n",
    "#                                 \"X.\", \"Th·ªã x√£\", \"Th·ªãX√£\", \"X√£\",\n",
    "#                                 \"Th·ªã tr·∫•n\", \"Th·ªãTr·∫•n\", \"T.T\",\n",
    "#                                 \"-\"\n",
    "#                                 ]\n",
    "#     vietnamese_dict = {\n",
    "#                 \"a\": [\"√†\", \"√°\", \"·∫°\", \"·∫£\", \"√£\", \"√¢\", \"·∫ß\", \"·∫•\", \"·∫≠\", \"·∫©\", \"·∫´\", \"ƒÉ\", \"·∫±\", \"·∫Ø\", \"·∫∑\", \"·∫≥\", \"·∫µ\"],\n",
    "#                 \"d\": [\"ƒë\"],\n",
    "#                 \"e\": [\"√®\", \"√©\", \"·∫π\", \"·∫ª\", \"·∫Ω\", \"√™\", \"·ªÅ\", \"·∫ø\", \"·ªá\", \"·ªÉ\", \"·ªÖ\"],\n",
    "#                 \"i\": [\"√¨\", \"√≠\", \"·ªã\", \"·ªâ\", \"ƒ©\"],\n",
    "#                 \"o\": [\"√≤\", \"√≥\", \"·ªç\", \"·ªè\", \"√µ\", \"√¥\", \"·ªì\", \"·ªë\", \"·ªô\", \"·ªï\", \"·ªó\", \"∆°\", \"·ªù\", \"·ªõ\", \"·ª£\", \"·ªü\", \"·ª°\"],\n",
    "#                 \"u\": [\"√π\", \"√∫\", \"·ª•\", \"·ªß\", \"≈©\", \"∆∞\", \"·ª´\", \"·ª©\", \"·ª±\", \"·ª≠\", \"·ªØ\"],\n",
    "#                 \"y\": [\"·ª≥\", \"√Ω\", \"·ªµ\", \"·ª∑\", \"·ªπ\"],\n",
    "#             }\n",
    "\n",
    "#     wrong_accents = {\n",
    "#                 \"o√†\": \"√≤a\", \"o√°\": \"√≥a\", \"o·∫°\": \"·ªça\", \"o√£\": \"√µa\", \"o·∫£\": \"·ªèa\",\n",
    "#                 \"qui\": \"quy\",\n",
    "#             }\n",
    "\n",
    "#     SPECIAL_CASES = [\"x√£\", \"x.\", \"huy·ªán\", \"t·ªânh\", \"t.\",\n",
    "#                     \"tp\", \"th√†nh ph·ªë\", \"th√†nhph·ªë\"]\n",
    "\n",
    "#     # Prefixes for wards and districts to expand possible matches\n",
    "#     DIGIT_CASES = {\n",
    "#         \"ward\": [\"p\", \"ph∆∞·ªùng\"],\n",
    "#         \"district\": [\"q\", \"qu·∫≠n\"],\n",
    "#     }\n",
    "\n",
    "#     # Dictionary to store generated variations for tracing back\n",
    "#     variation_map: Dict[str, dict] = {}\n",
    "\n",
    "#     province_short_form = {\n",
    "#         \"h·ªìch√≠minh\":\"hcm\",\n",
    "#         \"b√†r·ªãav≈©ngt√†u\":\"brvt\",\n",
    "#     }\n",
    "\n",
    "#     CATEGORY_PRIORITY = {\"province\": 1, \"district\": 2, \"ward\": 3}  # Lower number = higher priority\n",
    "#     COSINE_SIMILARITY_THRESHOLD = 0.75  # If similarity < 0.7, return \"null\", the less value - the less strict\n",
    "#     MAX_VALID_EDIT_DISTANCE = 3\n",
    "#     def __init__(self):\n",
    "#         # List for province, district, ward for private test, do not change for any reason (these files will be provided later with this exact name)\n",
    "#         self.province_path = 'list_province.txt'\n",
    "#         self.district_path = 'list_district.txt'\n",
    "#         self.ward_path = 'list_ward.txt'\n",
    "\n",
    "#         self.tries = {}\n",
    "#         self.variation_map = {}\n",
    "#         self.locality_map = {}\n",
    "#         self.current_id = 0\n",
    "#         self.reversed_vietnamese_dict = {}\n",
    "\n",
    "\n",
    "#         self.load_databases({\n",
    "#             \"province\": self.province_path,\n",
    "#             \"district\": self.district_path,\n",
    "#             \"ward\": self.ward_path\n",
    "#         })\n",
    "\n",
    "#     def common_normalize(self, text: str) -> str:\n",
    "#         text = text.lower()\n",
    "#         for case in self.SAFE_CASES:\n",
    "#             text = re.sub(re.escape(case), ',', text, flags=re.IGNORECASE)\n",
    "#         text = re.sub(r\"\\s{2,}\", \",\", text)  # Remove spaces\n",
    "#         text = text.replace(\" \", \"\")\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_and_remove_accent(self, text: str) -> str:\n",
    "#         \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "#         text = self.common_normalize(text)\n",
    "#         text = unicodedata.normalize(\"NFKD\", text)\n",
    "#         text = text.replace(\"ƒë\", \"d\")\n",
    "#         text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_but_keep_vietnamese_alphabet(self, text: str) -> str:\n",
    "#         text = self.common_normalize(text)\n",
    "#         for base_char, variations in vietnamese_dict.items():\n",
    "#             for char in variations:\n",
    "#                 self.reversed_vietnamese_dict[char] = base_char\n",
    "\n",
    "#         result = []\n",
    "#         for char in text:\n",
    "#             if char in self.reversed_vietnamese_dict:\n",
    "#                 result.append(self.reversed_vietnamese_dict[char])\n",
    "#             else:\n",
    "#                 result.append(char)\n",
    "\n",
    "#         text = \"\".join(result)\n",
    "#         text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "#         text = \"\".join(c for c in text if not unicodedata.combining(c))  # Remove accents\n",
    "#         text = re.sub(r\"\\s+\", \"\", text)  # Remove spaces\n",
    "#         return text\n",
    "\n",
    "#     def normalize_text_but_keep_accent(self, text: str) -> str:\n",
    "#         \"\"\"Normalize text by removing accents, spaces, and special cases.\"\"\"\n",
    "#         text = self.common_normalize(text)\n",
    "#         text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "#         for wrong, correct in wrong_accents.items():\n",
    "#             text = text.replace(wrong, correct)\n",
    "\n",
    "#         return text\n",
    "\n",
    "#     def segment_text(self, s, safe=True):\n",
    "#         text = s[:]\n",
    "\n",
    "#         prefixes = self.SAFE_CASES if safe else self.ALL_PREFIXES\n",
    "\n",
    "#         for p in prefixes:\n",
    "#             text = re.sub(re.escape(p), ',', text, flags=re.IGNORECASE)\n",
    "\n",
    "#         # Handle \"-\" in names (e.g., \"Ng-T-\" -> \"Ng T \")\n",
    "#         text = re.sub(r'[.\\-]', ' ', text)\n",
    "\n",
    "#         # Split address components\n",
    "#         segments = [seg.strip() for seg in text.split(',') if seg.strip()]\n",
    "#         return segments\n",
    "\n",
    "#     def generate_prefixed_variations(self, location_name: str, category: str) -> Tuple[List[str], str]:\n",
    "#         \"\"\"Generate prefixed variations ONLY for wards and districts, and store variations per category.\"\"\"\n",
    "#         variations = []\n",
    "\n",
    "#         if category not in self.variation_map:\n",
    "#           self.variation_map[category] = {}\n",
    "\n",
    "#         normalized_name = self.normalize_text_but_keep_accent(location_name)\n",
    "\n",
    "#         if normalized_name.isdigit():  # Only generate prefixes for wards and districts\n",
    "#             variations = [prefix + normalized_name for prefix in self.DIGIT_CASES[category]]\n",
    "#         elif normalized_name in province_short_form:\n",
    "#             variations = [normalized_name, province_short_form[normalized_name]]\n",
    "#         else:\n",
    "#             variations = [normalized_name]\n",
    "\n",
    "#         non_accents_variations = [self.normalize_text_and_remove_accent(variation) for variation in variations]\n",
    "#         variations.extend(non_accents_variations)\n",
    "\n",
    "#         # Store variations per category\n",
    "#         self.variation_map[category][normalized_name] = variations\n",
    "#         return variations, normalized_name\n",
    "\n",
    "#     def load_databases(self, filenames: Dict[str, str]) -> None:\n",
    "#         \"\"\"Load multiple database files into separate Tries with prefixed variations.\"\"\"\n",
    "#         for category, filename in filenames.items():\n",
    "#             trie = Trie()\n",
    "#             try:\n",
    "#                 with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "#                     for line in file:\n",
    "#                         self.load_line(line, trie, category)\n",
    "#                 self.tries[category] = trie\n",
    "#             except FileNotFoundError:\n",
    "#                 print(f\"Warning: File {filename} not found!\")\n",
    "#                 self.tries[category] = Trie()\n",
    "\n",
    "#     def load_line(self, line, trie, category):\n",
    "#         location_name = line.strip()\n",
    "#         if location_name == \"\":\n",
    "#             return\n",
    "\n",
    "#         prefixed_variations, normalized_text = self.generate_prefixed_variations(location_name, category)\n",
    "\n",
    "#         for variant in prefixed_variations:\n",
    "#             trie.original_names[variant] = location_name\n",
    "#             trie.insert(variant)\n",
    "\n",
    "#     # Cosine Similarity Function\n",
    "#     def cosine_similarity(self,s1, s2):\n",
    "#         vec1, vec2 = Counter(s1), Counter(s2)\n",
    "#         intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "#         dot_product = sum(vec1[x] * vec2[x] for x in intersection)\n",
    "#         magnitude1 = math.sqrt(sum(vec1[x] ** 2 for x in vec1.keys()))\n",
    "#         magnitude2 = math.sqrt(sum(vec2[x] ** 2 for x in vec2.keys()))\n",
    "#         return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0\n",
    "\n",
    "#         # Autocorrection with Category Priority, Damerau-Levenshtein Distance & Cosine Similarity Check\n",
    "#     def autocorrect(self, word_normalized, trie, category):\n",
    "#         best_distance = float(\"inf\")\n",
    "#         matches = []\n",
    "#         for candidate_normalized in trie.all_words:\n",
    "#             distance = editdistance.distance(word_normalized, candidate_normalized)\n",
    "\n",
    "#             # Prioritize lower edit distance & higher-ranked category (Province > District > Ward)\n",
    "#             if distance < min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "#                 matches = [candidate_normalized]\n",
    "#                 best_distance = distance\n",
    "#             elif distance == min(best_distance, MAX_VALID_EDIT_DISTANCE):\n",
    "#                 matches.append(candidate_normalized)\n",
    "\n",
    "#         # Check Cosine Similarity Threshold\n",
    "#         best_match = \"\"\n",
    "#         best_similarity = 0\n",
    "#         for match in matches:\n",
    "#             p = self.cosine_similarity(word_normalized, match)\n",
    "#             if p > COSINE_SIMILARITY_THRESHOLD and p > best_similarity:\n",
    "#                 best_similarity = p\n",
    "#                 best_match = match\n",
    "\n",
    "#         return best_match\n",
    "\n",
    "#     def search_locations_in_trie(self, tries, input_text, results):\n",
    "#         matched_positions = set()\n",
    "#         remaining_chars = list(input_text)\n",
    "\n",
    "#         for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "#             if results[category] != \"\":\n",
    "#                 continue\n",
    "#             match = self.search_part(tries[category], input_text, matched_positions, remaining_chars, reverse)\n",
    "#             res = match[0]\n",
    "#             if match and res:\n",
    "#                 input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "#             results[category] = res\n",
    "\n",
    "#         return results, input_text\n",
    "\n",
    "#     def search_locations_in_segments(self, tries, segments, results):\n",
    "#         for category, reverse in [(\"province\", True), (\"ward\", False), (\"district\", False)]:\n",
    "#             if results[category] != \"\":\n",
    "#                 continue\n",
    "\n",
    "#             res = self.search_in_segment(segments, tries[category], category, reverse)\n",
    "#             results[category] = res\n",
    "\n",
    "#         return results, segments\n",
    "\n",
    "#     def search_in_trie(self, trie, input_text, matched_positions, remaining_chars, reverse):\n",
    "#         match = self.search_part(trie, input_text, matched_positions, remaining_chars, reverse)\n",
    "#         res = match[0]\n",
    "#         if match and res:\n",
    "#             input_text = input_text[:match[1]] + \",\" + input_text[match[2]:]\n",
    "#         return res, input_text\n",
    "\n",
    "#     def search_in_segment(self, segments, trie, category, reverse=False):\n",
    "#         if reverse:\n",
    "#             segments.reverse()\n",
    "\n",
    "#         for seg in segments:\n",
    "#             word = self.autocorrect(seg, trie, category)\n",
    "#             if word == \"\":\n",
    "#                 continue\n",
    "#             segments.remove(seg)\n",
    "#             return word\n",
    "\n",
    "#         if reverse:\n",
    "#             segments.reverse()\n",
    "\n",
    "#         return \"\"\n",
    "\n",
    "#     def search_part(self, trie, input_text, matched_positions, remaining_chars, reversed=False):\n",
    "#         it_range = range(len(input_text))\n",
    "#         if reversed:\n",
    "#             it_range = it_range.__reversed__()\n",
    "#         for i in it_range:\n",
    "#             if i in matched_positions:\n",
    "#                 continue\n",
    "#             match = trie.search_max_length(input_text, i)\n",
    "#             if match:\n",
    "#                 return match\n",
    "#         return \"\", None, None\n",
    "\n",
    "#     def process(self, s: str):\n",
    "#       # Ghi l·∫°i chu·ªói ƒë·∫ßu v√†o ban ƒë·∫ßu\n",
    "#       s_copy = s[:]\n",
    "\n",
    "#       # Ti·∫øn h√†nh chia t√°ch v√† chu·∫©n h√≥a chu·ªói\n",
    "#       segments = self.segment_text(s)  # T√°ch c√°c ph·∫ßn c·ªßa ƒë·ªãa ch·ªâ\n",
    "#       input_text = self.normalize_text_but_keep_accent(\",\".join(segments))  # Chu·∫©n h√≥a m√† kh√¥ng lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát\n",
    "\n",
    "#       # Kh·ªüi t·∫°o k·∫øt qu·∫£ ban ƒë·∫ßu cho ward, district, province\n",
    "#       results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "\n",
    "#       # T√¨m ki·∫øm trong trie v·ªõi d·∫•u\n",
    "#       result, remaining_text = self.search_locations_in_trie(self.tries, input_text, results)\n",
    "\n",
    "#       # N·∫øu kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ·ªü t·ªânh/qu·∫≠n/ph∆∞·ªùng, th·ª≠ t√¨m ki·∫øm l·∫°i kh√¥ng c√≥ d·∫•u\n",
    "#       if result[\"province\"] == \"\" and result[\"district\"] == \"\" and result[\"ward\"] == \"\":\n",
    "#           remaining_text = self.normalize_text_and_remove_accent(remaining_text)\n",
    "#           result, remaining_text = self.search_locations_in_trie(self.tries, remaining_text, results)\n",
    "\n",
    "#       # N·∫øu v·∫´n ch∆∞a t√¨m th·∫•y, th·ª≠ t√¨m ki·∫øm theo c√°c ph·∫ßn c·ªßa chu·ªói (segments)\n",
    "#       if result[\"province\"] == \"\" and result[\"district\"] == \"\" and result[\"ward\"] == \"\":\n",
    "#           segments = self.segment_text(remaining_text, False)\n",
    "#           result, remaining_text = self.search_locations_in_segments(self.tries, segments, results)\n",
    "\n",
    "\n",
    "#       return {\n",
    "#           \"province\": self.tries[\"province\"].get_raw_text(result[\"province\"]),\n",
    "#           \"district\": self.tries[\"district\"].get_raw_text(result[\"district\"]),\n",
    "#           \"ward\": self.tries[\"ward\"].get_raw_text(result[\"ward\"]),\n",
    "#       }\n",
    "\n",
    "#     def debug_address(self, address: str):\n",
    "#         \"\"\"Debugging function to print step-by-step.\"\"\"\n",
    "#         print(\"\\nüìå Input Address:\", address)\n",
    "\n",
    "#         # Step 1: Normalize the address (remove spaces, special cases, etc.)\n",
    "#         preprocessed = self.normalize_text_but_keep_accent(address)\n",
    "#         print(\"üîπ After Preprocessing (with accents):\", preprocessed)\n",
    "\n",
    "#         # Step 2: Segment the address into components (e.g., province, district, ward)\n",
    "#         segments = self.segment_text(address)\n",
    "#         print(\"üîπ Address Segments:\", segments)\n",
    "\n",
    "#         # Step 3: Search for matches in the trie (with accents)\n",
    "#         results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "#         matched_results, remaining_text = self.search_locations_in_trie(self.tries, preprocessed, results)\n",
    "#         print(\"üîπ Extracted Matches (with accents):\", matched_results)\n",
    "#         print(\"üîπ Remaining Text After Accent Search:\", remaining_text)\n",
    "\n",
    "#         # Step 4: If no match found, try to remove accents and search again\n",
    "#         if matched_results[\"province\"] == \"\" and matched_results[\"district\"] == \"\" and matched_results[\"ward\"] == \"\":\n",
    "#             remaining_text_no_accents = self.normalize_text_and_remove_accent(remaining_text)\n",
    "#             matched_results_no_accents, remaining_text = self.search_locations_in_trie(self.tries, remaining_text_no_accents, results)\n",
    "#             print(\"üîπ Extracted Matches (without accents):\", matched_results_no_accents)\n",
    "#             print(\"üîπ Remaining Text After No-Accents Search:\", remaining_text)\n",
    "\n",
    "#         # Step 5: If still no matches, try searching by individual segments\n",
    "#         if matched_results[\"province\"] == \"\" and matched_results[\"district\"] == \"\" and matched_results[\"ward\"] == \"\":\n",
    "#             segments = self.segment_text(remaining_text, False)\n",
    "#             matched_results, remaining_text = self.search_locations_in_segments(self.tries, segments, results)\n",
    "#             print(\"üîπ Matches After Searching by Segments:\", matched_results)\n",
    "#             print(\"üîπ Remaining Text After Searching by Segments:\", remaining_text)\n",
    "\n",
    "#         # Step 6: Unnormalize the final results (convert normalized names back to original form)\n",
    "#         final_results = {\n",
    "#             \"province\": self.tries[\"province\"].get_raw_text(matched_results[\"province\"]),\n",
    "#             \"district\": self.tries[\"district\"].get_raw_text(matched_results[\"district\"]),\n",
    "#             \"ward\": self.tries[\"ward\"].get_raw_text(matched_results[\"ward\"]),\n",
    "#         }\n",
    "\n",
    "#         print(\"‚úÖ Final Address Components:\", final_results)\n",
    "#         return final_results\n",
    "\n",
    "\n",
    "class Solution:\n",
    "\n",
    "    debug=False\n",
    "\n",
    "    def __init__(self):\n",
    "        # list provice, district, ward for private test, do not change for any reason (these file will be provided later with this exact name)\n",
    "\n",
    "        self.province_path = 'list_province.txt'\n",
    "        self.district_path = 'list_district.txt'\n",
    "        self.ward_path = 'list_ward.txt'\n",
    "\n",
    "        self.tries = {}\n",
    "        load_databases({\n",
    "            \"province\": self.province_path,\n",
    "            \"district\": self.district_path,\n",
    "            \"ward\": self.ward_path\n",
    "        }, self.tries)\n",
    "\n",
    "        self.variation_map = variation_map\n",
    "        pass\n",
    "\n",
    "    def process(self, s: str):\n",
    "        # Preprocess\n",
    "        s_copy = s[:]\n",
    "\n",
    "        segments = segment_text(s)\n",
    "        input_text = normalize_text_but_keep_accent(\",\".join(segments))\n",
    "\n",
    "        # Start searching\n",
    "        results = {\"ward\": \"\", \"district\": \"\", \"province\": \"\"}\n",
    "\n",
    "        # Search with accents\n",
    "        result, remaining_text = search_locations_in_trie(self.tries, input_text, results)\n",
    "\n",
    "        # If the province/district/ward not found, search without accents\n",
    "        # remaining_text = normalize_text_and_remove_accent(remaining_text)\n",
    "        # result, remaining_text = search_locations_in_trie(self.tries, remaining_text, results)\n",
    "\n",
    "        # If the province/district/ward not found, search by segments\n",
    "        segments = segment_text(remaining_text, False)\n",
    "        result, remaining_text = search_locations_in_segments(self.tries, segments, results)\n",
    "\n",
    "        result =  {\n",
    "            \"province\": self.tries[\"province\"].get_raw_text(result[\"province\"]),\n",
    "            \"district\": self.tries[\"district\"].get_raw_text(result[\"district\"]),\n",
    "            \"ward\": self.tries[\"ward\"].get_raw_text(result[\"ward\"]),\n",
    "        }\n",
    "\n",
    "        if self.debug:\n",
    "            print()\n",
    "            print(f\"Original: {s_copy}\")\n",
    "            print(f\"Normalized: {normalize_text_but_keep_accent(s_copy)}\")\n",
    "            print(f\"Result: {result}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "# ƒêi·ªÅn H·∫£i -> ƒêi√™n H·∫£i -> Ti√™n H·∫£i\n",
    "# runner = Solution()\n",
    "# runner.debug = True\n",
    "\n",
    "# runner.process(\"Di√™n Th·∫°nh,,T Khabnh H√≤a\")\n",
    "\n",
    "\n",
    "# Not able to solve yet\n",
    "# runner.process(\" T.P Phan Rang-Th√°p lh√†m  Ninh Thu·∫≠n\")\n",
    "# runner.process(\"ƒêi√™n H·∫£i, ƒê√¥ng H·∫£i, T b·∫°c Li√™u\")\n",
    "\n"
   ],
   "metadata": {
    "id": "xtwG3tBDzMLD"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run tests"
   ],
   "metadata": {
    "id": "lLrJr9sLn4J3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CORRECT TESTS\n",
    "groups_province = {}\n",
    "groups_district = {'h√≤a b√¨nh': ['Ho√† B√¨nh', 'H√≤a B√¨nh'], 'kbang': ['Kbang', 'KBang'], 'quy nh∆°n': ['Qui Nh∆°n', 'Quy Nh∆°n']}\n",
    "groups_ward = {'√°i nghƒ©a': ['√°i Nghƒ©a', '√Åi Nghƒ©a'], '√°i qu·ªëc': ['√°i Qu·ªëc', '√Åi Qu·ªëc'], '√°i th∆∞·ª£ng': ['√°i Th∆∞·ª£ng', '√Åi Th∆∞·ª£ng'], '√°i t·ª≠': ['√°i T·ª≠', '√Åi T·ª≠'], '·∫•m h·∫°': ['·∫•m H·∫°', '·∫§m H·∫°'], 'an ·∫•p': ['An ·∫•p', 'An ·∫§p'], '·∫≥ng cang': ['·∫≥ng Cang', '·∫≤ng Cang'], '·∫≥ng n∆∞a': ['·∫≥ng N∆∞a', '·∫≤ng N∆∞a'], '·∫≥ng t·ªü': ['·∫≥ng T·ªü', '·∫≤ng T·ªü'], 'an h√≤a': ['An Ho√†', 'An H√≤a'], 'ayun': ['Ayun', 'AYun'], 'b·∫Øc √°i': ['B·∫Øc √°i', 'B·∫Øc √Åi'], 'b·∫£o √°i': ['B·∫£o √°i', 'B·∫£o √Åi'], 'b√¨nh h√≤a': ['B√¨nh Ho√†', 'B√¨nh H√≤a'], 'ch√¢u ·ªï': ['Ch√¢u ·ªï', 'Ch√¢u ·ªî'], 'ch∆∞ √°': ['Ch∆∞ √°', 'Ch∆∞ √Å'], 'ch∆∞ rcƒÉm': ['Ch∆∞ RcƒÉm', 'Ch∆∞ RCƒÉm'], 'c·ªông h√≤a': ['C·ªông Ho√†', 'C·ªông H√≤a'], 'c√≤ n√≤i': ['C√≤  N√≤i', 'C√≤ N√≤i'], 'ƒë·∫°i √¢n 2': ['ƒê·∫°i √Çn  2', 'ƒê·∫°i √Çn 2'], 'ƒëak ∆°': ['ƒêak ∆°', 'ƒêak ∆†'], \"ƒë·∫° m'ri\": [\"ƒê·∫° M'ri\", \"ƒê·∫° M'Ri\"], 'ƒë√¥ng h√≤a': ['ƒê√¥ng Ho√†', 'ƒê√¥ng H√≤a'], 'ƒë·ªìng √≠ch': ['ƒê·ªìng √≠ch', 'ƒê·ªìng √çch'], 'h·∫£i ch√¢u i': ['H·∫£i Ch√¢u  I', 'H·∫£i Ch√¢u I'], 'h·∫£i h√≤a': ['H·∫£i Ho√†', 'H·∫£i H√≤a'], 'h√†nh t√≠n ƒë√¥ng': ['H√†nh T√≠n  ƒê√¥ng', 'H√†nh T√≠n ƒê√¥ng'], 'hi·ªáp h√≤a': ['Hi·ªáp Ho√†', 'Hi·ªáp H√≤a'], 'h√≤a b·∫Øc': ['Ho√† B·∫Øc', 'H√≤a B·∫Øc'], 'h√≤a b√¨nh': ['Ho√† B√¨nh', 'H√≤a B√¨nh'], 'h√≤a ch√¢u': ['Ho√† Ch√¢u', 'H√≤a Ch√¢u'], 'h√≤a h·∫£i': ['Ho√† H·∫£i', 'H√≤a H·∫£i'], 'h√≤a hi·ªáp trung': ['Ho√† Hi·ªáp Trung', 'H√≤a Hi·ªáp Trung'], 'h√≤a li√™n': ['Ho√† Li√™n', 'H√≤a Li√™n'], 'h√≤a l·ªôc': ['Ho√† L·ªôc', 'H√≤a L·ªôc'], 'h√≤a l·ª£i': ['Ho√† L·ª£i', 'H√≤a L·ª£i'], 'h√≤a long': ['Ho√† Long', 'H√≤a Long'], 'h√≤a m·∫°c': ['Ho√† M·∫°c', 'H√≤a M·∫°c'], 'h√≤a minh': ['Ho√† Minh', 'H√≤a Minh'], 'h√≤a m·ªπ': ['Ho√† M·ªπ', 'H√≤a M·ªπ'], 'h√≤a ph√°t': ['Ho√† Ph√°t', 'H√≤a Ph√°t'], 'h√≤a phong': ['Ho√† Phong', 'H√≤a Phong'], 'h√≤a ph√∫': ['Ho√† Ph√∫', 'H√≤a Ph√∫'], 'h√≤a ph∆∞·ªõc': ['Ho√† Ph∆∞·ªõc', 'H√≤a Ph∆∞·ªõc'], 'h√≤a s∆°n': ['Ho√† S∆°n', 'H√≤a S∆°n'], 'h√≤a t√¢n': ['Ho√† T√¢n', 'H√≤a T√¢n'], 'h√≤a thu·∫≠n': ['Ho√† Thu·∫≠n', 'H√≤a Thu·∫≠n'], 'h√≤a ti·∫øn': ['Ho√† Ti·∫øn', 'H√≤a Ti·∫øn'], 'h√≤a tr·∫°ch': ['Ho√† Tr·∫°ch', 'H√≤a Tr·∫°ch'], 'h√≤a vinh': ['Ho√† Vinh', 'H√≤a Vinh'], 'h∆∞∆°ng h√≤a': ['H∆∞∆°ng Ho√†', 'H∆∞∆°ng H√≤a'], '√≠ch h·∫≠u': ['√≠ch H·∫≠u', '√çch H·∫≠u'], '√≠t ong': ['√≠t Ong', '√çt Ong'], 'kh√°nh h√≤a': ['Kh√°nh Ho√†', 'Kh√°nh H√≤a'], 'kr√¥ng √°': ['Kr√¥ng √Å', 'KR√¥ng √°'], 'l·ªôc h√≤a': ['L·ªôc Ho√†', 'L·ªôc H√≤a'], 'minh h√≤a': ['Minh Ho√†', 'Minh H√≤a'], 'm∆∞·ªùng ·∫£i': ['M∆∞·ªùng ·∫£i', 'M∆∞·ªùng ·∫¢i'], 'm∆∞·ªùng ·∫≥ng': ['M∆∞·ªùng ·∫≥ng', 'M∆∞·ªùng ·∫≤ng'], 'n·∫≠m √©t': ['N·∫≠m √©t', 'N·∫≠m √ât'], 'nam h√≤a': ['Nam Ho√†', 'Nam H√≤a'], 'na ∆∞': ['Na ∆∞', 'Na ∆Ø'], 'ng√£ s√°u': ['Ng√£ s√°u', 'Ng√£ S√°u'], 'nghi h√≤a': ['Nghi Ho√†', 'Nghi H√≤a'], 'nguy·ªÖn √∫y': ['Nguy·ªÖn U√Ω', 'Nguy·ªÖn √∫y', 'Nguy·ªÖn √öy'], 'nh√¢n h√≤a': ['Nh√¢n Ho√†', 'Nh√¢n H√≤a'], 'nh∆°n h√≤a': ['Nh∆°n Ho√†', 'Nh∆°n H√≤a'], 'nh∆°n nghƒ©a a': ['Nh∆°n nghƒ©a A', 'Nh∆°n Nghƒ©a A'], 'ph√∫c ·ª©ng': ['Ph√∫c ·ª©ng', 'Ph√∫c ·ª®ng'], 'ph∆∞·ªõc h√≤a': ['Ph∆∞·ªõc Ho√†', 'Ph∆∞·ªõc H√≤a'], 's∆°n h√≥a': ['S∆°n Ho√°', 'S∆°n H√≥a'], 't·∫° an kh∆∞∆°ng ƒë√¥ng': ['T·∫° An Kh∆∞∆°ng  ƒê√¥ng', 'T·∫° An Kh∆∞∆°ng ƒê√¥ng'], 't·∫° an kh∆∞∆°ng nam': ['T·∫° An Kh∆∞∆°ng  Nam', 'T·∫° An Kh∆∞∆°ng Nam'], 'tƒÉng h√≤a': ['TƒÉng Ho√†', 'TƒÉng H√≤a'], 't√¢n h√≤a': ['T√¢n Ho√†', 'T√¢n H√≤a'], 't√¢n h√≤a th√†nh': ['T√¢n H√≤a  Th√†nh', 'T√¢n H√≤a Th√†nh'], 't√¢n kh√°nh trung': ['T√¢n  Kh√°nh Trung', 'T√¢n Kh√°nh Trung'], 't√¢n l·ª£i': ['T√¢n l·ª£i', 'T√¢n L·ª£i'], 'th√°i h√≤a': ['Th√°i Ho√†', 'Th√°i H√≤a'], 'thi·∫øt ·ªëng': ['Thi·∫øt ·ªëng', 'Thi·∫øt ·ªêng'], 'thu·∫≠n h√≤a': ['Thu·∫≠n Ho√†', 'Thu·∫≠n H√≤a'], 'th∆∞·ª£ng ·∫•m': ['Th∆∞·ª£ng ·∫•m', 'Th∆∞·ª£ng ·∫§m'], 'th·ª•y h∆∞∆°ng': ['Thu·ªµ H∆∞∆°ng', 'Th·ª•y H∆∞∆°ng'], 'th·ªßy xu√¢n': ['Thu·ª∑ Xu√¢n', 'Th·ªßy Xu√¢n'], 't·ªãnh ·∫•n ƒë√¥ng': ['T·ªãnh ·∫•n ƒê√¥ng', 'T·ªãnh ·∫§n ƒê√¥ng'], 't·ªãnh ·∫•n t√¢y': ['T·ªãnh ·∫•n T√¢y', 'T·ªãnh ·∫§n T√¢y'], 'tri·ªáu √°i': ['Tri·ªáu √°i', 'Tri·ªáu √Åi'], 'tri·ªáu ·∫©u': ['Tri·ªáu ·∫©u', 'Tri·ªáu ·∫®u'], 'trung h√≤a': ['Trung Ho√†', 'Trung H√≤a'], 'trung √Ω': ['Trung √Ω', 'Trung √ù'], 't√πng ·∫£nh': ['T√πng ·∫£nh', 'T√πng ·∫¢nh'], '√∫c k·ª≥': ['√∫c K·ª≥', '√öc K·ª≥'], '·ª©ng h√≤e': ['·ª©ng Ho√®', '·ª®ng Ho√®'], 'vƒ©nh h√≤a': ['Vƒ©nh Ho√†', 'Vƒ©nh H√≤a'], 'v≈© h√≤a': ['V≈© Ho√†', 'V≈© H√≤a'], 'xu√¢n √°i': ['Xu√¢n √°i', 'Xu√¢n √Åi'], 'xu√¢n √°ng': ['Xu√¢n √°ng', 'Xu√¢n √Ång'], 'xu√¢n h√≤a': ['Xu√¢n Ho√†', 'Xu√¢n H√≤a'], 'xu·∫•t h√≥a': ['Xu·∫•t Ho√°', 'Xu·∫•t H√≥a'], '·ª∑ la': ['·ª∑ La', '·ª∂ La']}\n",
    "groups_ward.update({1: ['1', '01'], 2: ['2', '02'], 3: ['3', '03'], 4: ['4', '04'], 5: ['5', '05'], 6: ['6', '06'], 7: ['7', '07'], 8: ['8', '08'], 9: ['9', '09']})\n",
    "def to_same(groups):\n",
    "    same = {ele: k for k, v in groups.items() for ele in v}\n",
    "    return same\n",
    "same_province = to_same(groups_province)\n",
    "same_district = to_same(groups_district)\n",
    "same_ward = to_same(groups_ward)\n",
    "def normalize(text, same_dict):\n",
    "    return same_dict.get(text, text)"
   ],
   "metadata": {
    "id": "fyY3ymB8n6R7"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# NOTE: DO NOT change this cell\n",
    "# This cell is for downloading private test\n",
    "!rm -rf test.json\n",
    "# this link is public test\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB/view?usp=sharing -O test.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggOye_EFowUu",
    "outputId": "be0efa80-82c7-4c88-f9d5-72f657252697"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB\n",
      "To: /content/test.json\n",
      "\r  0% 0.00/79.4k [00:00<?, ?B/s]\r100% 79.4k/79.4k [00:00<00:00, 61.8MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "debug = True\n",
    "\n",
    "TEAM_NAME = 'GROUP4'  # This should be your team name\n",
    "EXCEL_FILE = f'{TEAM_NAME}.xlsx'\n",
    "\n",
    "import json\n",
    "import time\n",
    "with open('test.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "summary_only = True\n",
    "df = []\n",
    "solution = Solution()\n",
    "timer = []\n",
    "correct = 0\n",
    "for test_idx, data_point in enumerate(data):\n",
    "    address = data_point[\"text\"]\n",
    "\n",
    "    ok = 0\n",
    "    try:\n",
    "        answer = data_point[\"result\"]\n",
    "        answer[\"province_normalized\"] = normalize(answer[\"province\"], same_province)\n",
    "        answer[\"district_normalized\"] = normalize(answer[\"district\"], same_district)\n",
    "        answer[\"ward_normalized\"] = normalize(answer[\"ward\"], same_ward)\n",
    "\n",
    "        start = time.perf_counter_ns()\n",
    "        result = solution.process(address)\n",
    "        finish = time.perf_counter_ns()\n",
    "        timer.append(finish - start)\n",
    "        result[\"province_normalized\"] = normalize(result[\"province\"], same_province)\n",
    "        result[\"district_normalized\"] = normalize(result[\"district\"], same_district)\n",
    "        result[\"ward_normalized\"] = normalize(result[\"ward\"], same_ward)\n",
    "\n",
    "        province_correct = int(answer[\"province_normalized\"] == result[\"province_normalized\"])\n",
    "        district_correct = int(answer[\"district_normalized\"] == result[\"district_normalized\"])\n",
    "        ward_correct = int(answer[\"ward_normalized\"] == result[\"ward_normalized\"])\n",
    "        ok = province_correct + district_correct + ward_correct\n",
    "\n",
    "        df.append([\n",
    "            test_idx,\n",
    "            address,\n",
    "            answer[\"province\"],\n",
    "            result[\"province\"],\n",
    "            answer[\"province_normalized\"],\n",
    "            result[\"province_normalized\"],\n",
    "            province_correct,\n",
    "            answer[\"district\"],\n",
    "            result[\"district\"],\n",
    "            answer[\"district_normalized\"],\n",
    "            result[\"district_normalized\"],\n",
    "            district_correct,\n",
    "            answer[\"ward\"],\n",
    "            result[\"ward\"],\n",
    "            answer[\"ward_normalized\"],\n",
    "            result[\"ward_normalized\"],\n",
    "            ward_correct,\n",
    "            ok,\n",
    "            timer[-1] / 1_000_000_000,\n",
    "        ])\n",
    "        if debug and ok < 3:\n",
    "          print()\n",
    "          print(\"Original: \" + address)\n",
    "          if province_correct == 0:\n",
    "              print(f\"Province -> Result: '{result['province']}', Answer: '{answer['province']}'\")\n",
    "          if district_correct == 0:\n",
    "              print(f\"District -> Result: '{result['district']}', Answer: '{answer['district']}'\")\n",
    "          if ward_correct == 0:\n",
    "              print(f\"Ward -> Result: '{result['ward']}', Answer: '{answer['ward']}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{answer = }\")\n",
    "        print(f\"{result = }\")\n",
    "        df.append([\n",
    "            test_idx,\n",
    "            address,\n",
    "            answer[\"province\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"province_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            answer[\"district\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"district_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            answer[\"ward\"],\n",
    "            \"EXCEPTION\",\n",
    "            answer[\"ward_normalized\"],\n",
    "            \"EXCEPTION\",\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])\n",
    "        # any failure count as a zero correct\n",
    "        pass\n",
    "    correct += ok\n",
    "\n",
    "\n",
    "    if not summary_only:\n",
    "        # responsive stuff\n",
    "        print(f\"Test {test_idx:5d}/{len(data):5d}\")\n",
    "        print(f\"Correct: {ok}/3\")\n",
    "        print(f\"Time Executed: {timer[-1] / 1_000_000_000:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"-\"*30)\n",
    "total = len(data) * 3\n",
    "score_scale_10 = round(correct / total * 10, 2)\n",
    "if len(timer) == 0:\n",
    "    timer = [0]\n",
    "max_time_sec = round(max(timer) / 1_000_000_000, 4)\n",
    "avg_time_sec = round((sum(timer) / len(timer)) / 1_000_000_000, 4)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    [[correct, total, score_scale_10, max_time_sec, avg_time_sec]],\n",
    "    columns=['correct', 'total', 'score / 10', 'max_time_sec', 'avg_time_sec',],\n",
    ")\n",
    "\n",
    "columns = [\n",
    "    'ID',\n",
    "    'text',\n",
    "    'province',\n",
    "    'province_student',\n",
    "    'province_normalized',\n",
    "    'province_student_normalized',\n",
    "    'province_correct',\n",
    "    'district',\n",
    "    'district_student',\n",
    "    'district_normalized',\n",
    "    'district_student_normalized',\n",
    "    'district_correct',\n",
    "    'ward',\n",
    "    'ward_student',\n",
    "    'ward_normalized',\n",
    "    'ward_student_normalized',\n",
    "    'ward_correct',\n",
    "    'total_correct',\n",
    "    'time_sec',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = columns\n",
    "\n",
    "print(df2)\n",
    "\n",
    "!pip install xlsxwriter\n",
    "\n",
    "if not debug:\n",
    "    print(f'{TEAM_NAME = }')\n",
    "    print(f'{EXCEL_FILE = }')\n",
    "    writer = pd.ExcelWriter(EXCEL_FILE, engine='xlsxwriter')\n",
    "    df2.to_excel(writer, index=False, sheet_name='summary')\n",
    "    df.to_excel(writer, index=False, sheet_name='details')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# Th√°i H√≤a, H·ª£p ƒê·ªìng Ch∆∞∆°ng M·ªπ, H√† N·ªôi\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZZNgQSwn-LK",
    "outputId": "c8873d2e-5620-4276-961b-9a108f1e3bc4"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Original: 357/28,Ng-T- Thu·∫≠t,P1,Q3,TP.H·ªìCh√≠Minh.\n",
      "Ward -> Result: '12', Answer: ''\n",
      "\n",
      "Original: 284DBis Ng VƒÉn Gi√°o, P3, M·ªπ Tho, T.Giang.\n",
      "Province -> Result: 'An Giang', Answer: 'Ti·ªÅn Giang'\n",
      "\n",
      "Original: Ti·ªÉu khu 3, th·ªã tr·∫•n Ba H√†ng, huy·ªán Ph·ªï Y√™n, t·ªânh Th√°i Nguy√™n.\n",
      "Province -> Result: 'Th√°i Nguy√™n', Answer: ''\n",
      "District -> Result: 'Ph·ªï Y√™n', Answer: ''\n",
      "\n",
      "Original: 154/4/81 Nguy·ªÖn - Ph√∫c Chu, P15, TB, TP. H·ªì Ch√≠ Minh\n",
      "Province -> Result: 'H·ªì Ch√≠ Minh', Answer: ''\n",
      "Ward -> Result: 'Ch·ª£ Chu', Answer: ''\n",
      "\n",
      "Original: T·ªï 8, ·∫§p An B√¨nh Minh H√≤a, Ch√¢u Th√†nh, Ki√™n Giang\n",
      "Ward -> Result: 'B√¨nh Minh', Answer: 'Minh H√≤a'\n",
      "\n",
      "Original: Ph·ªë ƒê·ª©c S∆°n, TT B√∫t S∆°n, Ho·∫±ng Ho√°, Thanh Ho√°.\n",
      "Ward -> Result: 'ƒê·ª©c S∆°n', Answer: 'B√∫t S∆°n'\n",
      "\n",
      "Original: 285 B/1A B√¨nh Gƒ©a Ph∆∞·ªùng 8,V≈©ng T√†u,B√† R·ªãa - V≈©ng T√†u\n",
      "Province -> Result: '', Answer: 'B√† R·ªãa - V≈©ng T√†u'\n",
      "\n",
      "Original: KP Ph∆∞·ªõc Trung Th·ªã tr·∫•n ƒê·∫•t ƒê·ªè, ƒê·∫•t ƒê·ªè, B√† R·ªãa - V≈©ng T√†u\n",
      "Province -> Result: '', Answer: 'B√† R·ªãa - V≈©ng T√†u'\n",
      "\n",
      "Original: 55/30 Nguy·ªÖn - Th∆∞·ª£ng Hi·ªÅn,P5,BT, TP. H·ªì Ch√≠ Minh\n",
      "District -> Result: '', Answer: 'B√¨nh Th·∫°nh'\n",
      "\n",
      "Original: Th√°i H√≤a, H·ª£p ƒê·ªìng Ch∆∞∆°ng M·ªπ, H√† N·ªôi\n",
      "Ward -> Result: 'Th√°i H√≤a', Answer: 'H·ª£p ƒê·ªìng'\n",
      "\n",
      "Original: Khu ph·ªë Nam T√¢n, TT Thu·∫≠n Nam, H√†m Thu·∫≠n B·∫Øc, B√¨nh Thu·∫≠n.\n",
      "Province -> Result: 'B√¨nh Thu·∫≠n', Answer: ''\n",
      "Ward -> Result: 'N·∫≠m Tin', Answer: ''\n",
      "\n",
      "Original: 2 d√£y C ng√µ 16 Ng√¥ Quy·ªÅn, t·ªï 14, Quang Trung, H√† ƒê√¥ng, H√† N·ªôi\n",
      "Ward -> Result: 'Ng√¥ Quy·ªÅn', Answer: 'Quang Trung'\n",
      "\n",
      "Original: Long B√¨nh 1, An H·∫£i, Ninh Ph∆∞·ªõc, Ninh Thu·∫≠n\n",
      "Ward -> Result: 'Long B√¨nh', Answer: 'An H·∫£i'\n",
      "\n",
      "Original: - Khu B Chu Ho√†, Vi·ªát HhiPh√∫ Th·ªç\n",
      "Province -> Result: 'Ph√∫ Th·ªç', Answer: ''\n",
      "\n",
      "Original: A:12A.21BlockA C/c BCA,P.AnKh√°nh,TP.Th·ªß ƒê·ª©c, TP. HCM\n",
      "Ward -> Result: 'Xu√¢n Khanh', Answer: ''\n",
      "\n",
      "Original: th√¥n 2 Su·ªëi Rao, Ch√¢u ƒê·ª©c, B√† R·ªãa - V≈©ng T√†u\n",
      "Province -> Result: '', Answer: 'B√† R·ªãa - V≈©ng T√†u'\n",
      "\n",
      "Original: Long B√¨nh, An H·∫£i, Ninh Ph∆∞·ªõc, Ninh Thu·∫≠n\n",
      "Ward -> Result: 'Long B√¨nh', Answer: 'An H·∫£i'\n",
      "\n",
      "Original: S·ªë 1410 ƒê∆∞·ªùng 30/4, Ph∆∞·ªùng 12, Th√†nh ph·ªë V≈©ng T√†u, B√† R·ªãa - V≈©ng T√†u.\n",
      "Province -> Result: '', Answer: 'B√† R·ªãa - V≈©ng T√†u'\n",
      "\n",
      "Original: F2B PhanVƒÉnTr P.5,GV,TP.H·ªì Ch√≠ Minh\n",
      "Ward -> Result: '5', Answer: ''\n",
      "\n",
      "Original: X Cam B√¨nh, Th√†nh Fh·ªëCam Rah,  Kh√°nh H√≤a\n",
      "District -> Result: 'Thanh H√†', Answer: 'Cam Ranh'\n",
      "\n",
      "Original:  Minh T√¢n,h.L∆∞ng Tai,\n",
      "District -> Result: '', Answer: 'L∆∞∆°ng T√†i'\n",
      "\n",
      "Original:  Huy·ªán Hi√™p H√≤a B·∫Øc Giang\n",
      "District -> Result: '', Answer: 'Hi·ªáp H√≤a'\n",
      "Ward -> Result: 'Minh H√≤a', Answer: ''\n",
      "\n",
      "Original:   ƒê√¥ng Giang T Qu·∫£yg Nm\n",
      "District -> Result: '', Answer: 'ƒê√¥ng Giang'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: ''\n",
      "\n",
      "Original: X√£V Ninh, Ki√™n X∆∞∆°ng,\n",
      "Province -> Result: 'Ki√™n Giang', Answer: ''\n",
      "District -> Result: '', Answer: 'Ki·∫øn X∆∞∆°ng'\n",
      "\n",
      "Original: Ph∆∞ng Kh√¢m Thi√™n Qu·∫≠n ƒê.ƒêa T.Ph·ªë H√†N·ªôi\n",
      "District -> Result: 'H∆∞ng H√†', Answer: 'ƒê·ªëng ƒêa'\n",
      "\n",
      "Original: XVnh Hung, , T.Thanh H√≥a\n",
      "Ward -> Result: 'Vƒ©nh H∆∞ng', Answer: 'Vƒ©nh H√πng'\n",
      "\n",
      "Original: Ph∆∞ong Th·ªç Son,T.Ph·ªë Vi·ªát Tr√¨, Ph√∫ Th·ªç\n",
      "Ward -> Result: '', Answer: 'Th·ªç S∆°n'\n",
      "\n",
      "Original: X.Ch√™ La,,T.H√† GianZ\n",
      "Ward -> Result: '', Answer: 'Ch·∫ø L√†'\n",
      "\n",
      "Original: H√¥no S·ªπ  T·ªânh Cao B·∫±ng\n",
      "Ward -> Result: '', Answer: 'H·ªìng S·ªπ'\n",
      "\n",
      "Original: Fm·ªÖ Tr√¨, Nam T·ª´ Li√™m, Th√†nh ph·ªë HN\n",
      "Province -> Result: '', Answer: 'H√† N·ªôi'\n",
      "\n",
      "Original:  Long Th·∫Øng, Huyn Lai ung, \n",
      "District -> Result: '', Answer: 'Lai Vung'\n",
      "\n",
      "Original: X. T√¢n An, Huy·ªánThnh H√†, \n",
      "Province -> Result: 'Thanh H√≥a', Answer: ''\n",
      "District -> Result: '', Answer: 'Thanh H√†'\n",
      "\n",
      "Original: X√£ N√¢m Nt·ª´  N·∫≠m P·ªì \n",
      "Ward -> Result: '', Answer: 'N·∫≠m Nh·ª´'\n",
      "\n",
      "Original: ,HPh√∫ B√¨nh, th√°i Nguy√™n\n",
      "District -> Result: '', Answer: 'Ph√∫ B√¨nh'\n",
      "Ward -> Result: 'Ph√∫ B√¨nh', Answer: ''\n",
      "\n",
      "Original:  Kh·∫£ c·ª≠u, H. Thanh Srn, \n",
      "Province -> Result: 'Thanh H√≥a', Answer: ''\n",
      "District -> Result: '', Answer: 'Thanh S∆°n'\n",
      "\n",
      "Original: , ƒê√¥ng H√≤a,T·ªânh Ph√∫ yn\n",
      "District -> Result: 'Ph√∫ Ho√†', Answer: 'ƒê√¥ng H√≤a'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: ''\n",
      "\n",
      "Original: Xa S∆°n Thy H. Le Th·ªßy T·ªânh Qu·∫£ng B√¨nh\n",
      "Ward -> Result: '', Answer: 'S∆°n Th·ªßy'\n",
      "\n",
      "Original:  ƒê√¥ng Ht√≤a Phu Y√™n\n",
      "Province -> Result: '', Answer: 'Ph√∫ Y√™n'\n",
      "District -> Result: '', Answer: 'ƒê√¥ng H√≤a'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: ''\n",
      "\n",
      "Original: , ThƒÉngFB√¨h,T. Qu·∫£ng Nam\n",
      "District -> Result: '', Answer: 'ThƒÉng B√¨nh'\n",
      "\n",
      "Original: X.H M·ªó, ƒêan PhT∆∞·ª£ng,\n",
      "Ward -> Result: '', Answer: 'H·∫° M·ªó'\n",
      "\n",
      "Original: X. L·ªôc Thah,T0P B·∫£o l·ªôc,TL√¢m ƒê√¥ng\n",
      "Province -> Result: '', Answer: 'L√¢m ƒê·ªìng'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: 'L·ªôc Thanh'\n",
      "\n",
      "Original: x√£ V·∫°n Kim, Huy·ªánM·ªπ ƒê·ª©c, T.P HN·ªôi\n",
      "Province -> Result: '', Answer: 'H√† N·ªôi'\n",
      "\n",
      "Original: X Phu M√£n   HN\n",
      "Province -> Result: '', Answer: 'H√† N·ªôi'\n",
      "\n",
      "Original: T√≠ch S∆°n T.Ph·ªëvinh Y√™n t vƒ©nh ph√∫c\n",
      "District -> Result: '', Answer: 'Vƒ©nh Y√™n'\n",
      "\n",
      "Original: X.H√≤a Tr·ªã Huzyen h√∫ Hoa TPh√∫ Y√™n\n",
      "District -> Result: '', Answer: 'Ph√∫ Ho√†'\n",
      "\n",
      "Original: ,H.Tr·∫£ng Bom,ƒê·ªìng Nai\n",
      "District -> Result: '', Answer: 'Tr·∫£ng Bom'\n",
      "Ward -> Result: 'Tr·∫£ng Bom', Answer: ''\n",
      "\n",
      "Original: X. S∆°n Hv HS∆°n Hoa T·ªânh Ph√∫ Y√™n\n",
      "District -> Result: '', Answer: 'S∆°n H√≤a'\n",
      "Ward -> Result: '', Answer: 'S∆°n H·ªôi'\n",
      "\n",
      "Original: X√£ T√¢n Ki√™n Hb√¨nh Ch√°nh Th√†nh ph√¥H√¥Ch√≠Minh\n",
      "Province -> Result: '', Answer: 'H·ªì Ch√≠ Minh'\n",
      "\n",
      "Original: X. Tam ƒê·ªìng,Huy·ªánM√™ Linh,T.Phw H√†Noi\n",
      "Province -> Result: '', Answer: 'H√† N·ªôi'\n",
      "\n",
      "Original:  eƒÉn Ti·∫øn, Y√™n L·∫°c, vƒ©nh P0h√∫c\n",
      "District -> Result: '', Answer: 'Y√™n L·∫°c'\n",
      "Ward -> Result: 'Y√™n L·∫°c', Answer: 'VƒÉn Ti·∫øn'\n",
      "\n",
      "Original: Di√™n Th·∫°nh,,T Khabnh H√≤a\n",
      "Province -> Result: 'Thanh H√≥a', Answer: 'Kh√°nh H√≤a'\n",
      "\n",
      "Original:  Qu·∫≠n 1 T.P H.C.Minh\n",
      "Ward -> Result: 'V≈© Ninh', Answer: ''\n",
      "\n",
      "Original: ,H.Tuy An,Tinh Ph√∫ y√™n\n",
      "Ward -> Result: 'T√¢n Lƒ©nh', Answer: ''\n",
      "\n",
      "Original:  T.P Phan Rang-Th√°p lh√†m  Ninh Thu·∫≠n\n",
      "District -> Result: '', Answer: 'Phan Rang-Th√°p Ch√†m'\n",
      "\n",
      "Original:  Th√°i Ha, HBa V√¨, T.pHN·ªôi\n",
      "Province -> Result: '', Answer: 'H√† N·ªôi'\n",
      "\n",
      "Original: , Nam ƒê√¥ng,T. T.T.H\n",
      "Province -> Result: 'H√† Nam', Answer: 'Th·ª´a Thi√™n Hu·∫ø'\n",
      "District -> Result: '', Answer: 'Nam ƒê√¥ng'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: ''\n",
      "\n",
      "Original:  Cam Nh√¢n Huy·ªánY√™n B√¨nh TY√™n B√°i\n",
      "District -> Result: '', Answer: 'Y√™n B√¨nh'\n",
      "Ward -> Result: 'Y√™n B√¨nh', Answer: 'C·∫£m Nh√¢n'\n",
      "\n",
      "Original:  ƒêi√™n H·∫£i, ƒê√¥ng H·∫£i, T b·∫°c Li√™u\n",
      "District -> Result: 'Ti·ªÅn H·∫£i', Answer: 'ƒê√¥ng H·∫£i'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: 'ƒêi·ªÅn H·∫£i'\n",
      "\n",
      "Original: ƒê√¥ng thanh AnE Minh T kien Giang\n",
      "District -> Result: '', Answer: 'An Minh'\n",
      "Ward -> Result: 'ƒê√¥ng', Answer: 'ƒê√¥ng Th·∫°nh'\n",
      "\n",
      "Original: P Th·ªßy Ch√¢u,T.X. h∆∞∆°ngTh·ªßy,Th·ª´a.t.Hu·∫ø\n",
      "Province -> Result: '', Answer: 'Th·ª´a Thi√™n Hu·∫ø'\n",
      "------------------------------\n",
      "   correct  total  score / 10  max_time_sec  avg_time_sec\n",
      "0     1269   1350         9.4        0.0311        0.0023\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m165.1/165.1 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.2\n"
     ]
    }
   ]
  }
 ]
}
